{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El código se está ejecutando en un entorno local.\n"
     ]
    }
   ],
   "source": [
    "# Comprueba si el código se está ejecutando en Google Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "\n",
    "path_absolute = ''\n",
    "if IN_COLAB:\n",
    "    print(\"El código se está ejecutando en Google Colab.\")\n",
    "    from google.colab import drive\n",
    "\n",
    "    drive.mount('/content/drive')\n",
    "    path_absolute = '/content/drive/Othercomputers/Mi_portátil/TFM/WorkSpace/'\n",
    "\n",
    "    # Cambia al directorio de tu carpeta en Google Drive\n",
    "    os.chdir(path_absolute)\n",
    "\n",
    "    # Lista los archivos y carpetas en el directorio actual\n",
    "    contenido_carpeta = os.listdir(path_absolute)\n",
    "    print(\"Contenido de la carpeta en Google Drive:\")\n",
    "    print(contenido_carpeta)\n",
    "else:\n",
    "    print(\"El código se está ejecutando en un entorno local.\")\n",
    "    path_absolute = 'C:/Users/jaime/OneDrive - Universidad de Málaga/Escritorio/UNIR/TFM/WorkSpace/'\n",
    "\n",
    "path_file = 'df_movies_rating_2.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(path_absolute)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#-------------------------------------------------------------------------------------\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Input, Embedding, Flatten, Dense, Concatenate\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "#-------------------------------------------------------------------------------------\n",
    "import Utils.utils as util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFrame = pd.read_csv(path_absolute+path_file, sep=',')\n",
    "data = dataFrame.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Especifica el tamaño de la muestra que deseas obtener\n",
    "tamano_muestra = 500  # Cambia esto al tamaño de muestra que desees\n",
    "# Obtiene una muestra aleatoria uniforme del DataFrame\n",
    "data = data.sample(n=tamano_muestra, random_state=42)  # random_state para reproducibilidad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 500 entries, 550126 to 190092\n",
      "Data columns (total 28 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   userId              500 non-null    int64  \n",
      " 1   movieId             500 non-null    int64  \n",
      " 2   timestamp_scr       500 non-null    int64  \n",
      " 3   tag                 500 non-null    object \n",
      " 4   tag_encoded         500 non-null    int64  \n",
      " 5   timestamp_tags      500 non-null    int64  \n",
      " 6   title               500 non-null    object \n",
      " 7   (no genres listed)  500 non-null    int64  \n",
      " 8   Action              500 non-null    int64  \n",
      " 9   Adventure           500 non-null    int64  \n",
      " 10  Animation           500 non-null    int64  \n",
      " 11  Children            500 non-null    int64  \n",
      " 12  Comedy              500 non-null    int64  \n",
      " 13  Crime               500 non-null    int64  \n",
      " 14  Documentary         500 non-null    int64  \n",
      " 15  Drama               500 non-null    int64  \n",
      " 16  Fantasy             500 non-null    int64  \n",
      " 17  Film-Noir           500 non-null    int64  \n",
      " 18  Horror              500 non-null    int64  \n",
      " 19  IMAX                500 non-null    int64  \n",
      " 20  Musical             500 non-null    int64  \n",
      " 21  Mystery             500 non-null    int64  \n",
      " 22  Romance             500 non-null    int64  \n",
      " 23  Sci-Fi              500 non-null    int64  \n",
      " 24  Thriller            500 non-null    int64  \n",
      " 25  War                 500 non-null    int64  \n",
      " 26  Western             500 non-null    int64  \n",
      " 27  rating              500 non-null    float64\n",
      "dtypes: float64(1), int64(25), object(2)\n",
      "memory usage: 113.3+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quito las calumnas categoricas de título y tag ambas estan represnetadas en las columnas númericas de tag_encode e id movie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# util.contar_ocurrencias(data, '(no genres listed)')\n",
    "# util.mostrar_filas_por_valor(data, '(no genres listed)', 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total de clases(Puntuaciones) a predecir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valor: 5.0, Frecuencia: 112\n",
      "Valor: 4.0, Frecuencia: 105\n",
      "Valor: 4.5, Frecuencia: 90\n",
      "Valor: 3.5, Frecuencia: 86\n",
      "Valor: 3.0, Frecuencia: 51\n",
      "Valor: 2.5, Frecuencia: 20\n",
      "Valor: 2.0, Frecuencia: 18\n",
      "Valor: 1.0, Frecuencia: 8\n",
      "Valor: 1.5, Frecuencia: 7\n",
      "Valor: 0.5, Frecuencia: 3\n",
      "Total de valores distintos: 10\n"
     ]
    }
   ],
   "source": [
    "data = util.eliminar_columnas(data, ['tag', 'title', '(no genres listed)'])\n",
    "util.contar_ocurrencias(data, 'rating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 500 entries, 550126 to 190092\n",
      "Data columns (total 25 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   userId          500 non-null    int64  \n",
      " 1   movieId         500 non-null    int64  \n",
      " 2   timestamp_scr   500 non-null    int64  \n",
      " 3   tag_encoded     500 non-null    int64  \n",
      " 4   timestamp_tags  500 non-null    int64  \n",
      " 5   Action          500 non-null    int64  \n",
      " 6   Adventure       500 non-null    int64  \n",
      " 7   Animation       500 non-null    int64  \n",
      " 8   Children        500 non-null    int64  \n",
      " 9   Comedy          500 non-null    int64  \n",
      " 10  Crime           500 non-null    int64  \n",
      " 11  Documentary     500 non-null    int64  \n",
      " 12  Drama           500 non-null    int64  \n",
      " 13  Fantasy         500 non-null    int64  \n",
      " 14  Film-Noir       500 non-null    int64  \n",
      " 15  Horror          500 non-null    int64  \n",
      " 16  IMAX            500 non-null    int64  \n",
      " 17  Musical         500 non-null    int64  \n",
      " 18  Mystery         500 non-null    int64  \n",
      " 19  Romance         500 non-null    int64  \n",
      " 20  Sci-Fi          500 non-null    int64  \n",
      " 21  Thriller        500 non-null    int64  \n",
      " 22  War             500 non-null    int64  \n",
      " 23  Western         500 non-null    int64  \n",
      " 24  rating          500 non-null    float64\n",
      "dtypes: float64(1), int64(24)\n",
      "memory usage: 101.6 KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>timestamp_scr</th>\n",
       "      <th>tag_encoded</th>\n",
       "      <th>timestamp_tags</th>\n",
       "      <th>Action</th>\n",
       "      <th>Adventure</th>\n",
       "      <th>Animation</th>\n",
       "      <th>Children</th>\n",
       "      <th>Comedy</th>\n",
       "      <th>...</th>\n",
       "      <th>Horror</th>\n",
       "      <th>IMAX</th>\n",
       "      <th>Musical</th>\n",
       "      <th>Mystery</th>\n",
       "      <th>Romance</th>\n",
       "      <th>Sci-Fi</th>\n",
       "      <th>Thriller</th>\n",
       "      <th>War</th>\n",
       "      <th>Western</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>550126</th>\n",
       "      <td>64333</td>\n",
       "      <td>42197</td>\n",
       "      <td>1388895420</td>\n",
       "      <td>31405</td>\n",
       "      <td>1388895075</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>722840</th>\n",
       "      <td>15078</td>\n",
       "      <td>3993</td>\n",
       "      <td>1196565802</td>\n",
       "      <td>2699</td>\n",
       "      <td>1296956745</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>606031</th>\n",
       "      <td>141263</td>\n",
       "      <td>5459</td>\n",
       "      <td>1350545592</td>\n",
       "      <td>9448</td>\n",
       "      <td>1350627511</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49993</th>\n",
       "      <td>133400</td>\n",
       "      <td>134130</td>\n",
       "      <td>1455373334</td>\n",
       "      <td>4293</td>\n",
       "      <td>1468601068</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122599</th>\n",
       "      <td>18057</td>\n",
       "      <td>4226</td>\n",
       "      <td>1166049878</td>\n",
       "      <td>1582</td>\n",
       "      <td>1166050137</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        userId  movieId  timestamp_scr  tag_encoded  timestamp_tags  Action  \\\n",
       "550126   64333    42197     1388895420        31405      1388895075       0   \n",
       "722840   15078     3993     1196565802         2699      1296956745       0   \n",
       "606031  141263     5459     1350545592         9448      1350627511       1   \n",
       "49993   133400   134130     1455373334         4293      1468601068       0   \n",
       "122599   18057     4226     1166049878         1582      1166050137       0   \n",
       "\n",
       "        Adventure  Animation  Children  Comedy  ...  Horror  IMAX  Musical  \\\n",
       "550126          0          0         0       1  ...       0     0        0   \n",
       "722840          0          0         0       0  ...       0     0        0   \n",
       "606031          0          0         0       1  ...       0     0        0   \n",
       "49993           1          0         0       0  ...       0     0        0   \n",
       "122599          0          0         0       0  ...       0     0        0   \n",
       "\n",
       "        Mystery  Romance  Sci-Fi  Thriller  War  Western  rating  \n",
       "550126        0        0       0         0    0        0     1.5  \n",
       "722840        0        1       0         0    0        0     5.0  \n",
       "606031        0        0       1         0    0        0     1.0  \n",
       "49993         0        0       1         0    0        0     5.0  \n",
       "122599        1        0       0         1    0        0     5.0  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODELO CON KERAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "5/5 [==============================] - 3s 250ms/step - loss: 15097519788785664.0000 - mean_absolute_error: 110414296.0000 - val_loss: 32972023529472.0000 - val_mean_absolute_error: 5217486.5000\n",
      "Epoch 2/10\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 2309804957630464.0000 - mean_absolute_error: 41150912.0000 - val_loss: 5152445705486336.0000 - val_mean_absolute_error: 71278992.0000\n",
      "Epoch 3/10\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 4104552873721856.0000 - mean_absolute_error: 63058092.0000 - val_loss: 1168206140014592.0000 - val_mean_absolute_error: 33783188.0000\n",
      "Epoch 4/10\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 405289488613376.0000 - mean_absolute_error: 16531237.0000 - val_loss: 488591822159872.0000 - val_mean_absolute_error: 21908050.0000\n",
      "Epoch 5/10\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 1009971022004224.0000 - mean_absolute_error: 31130252.0000 - val_loss: 1178191938977792.0000 - val_mean_absolute_error: 34147180.0000\n",
      "Epoch 6/10\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 679030839836672.0000 - mean_absolute_error: 24745952.0000 - val_loss: 56946254151680.0000 - val_mean_absolute_error: 7030646.5000\n",
      "Epoch 7/10\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 92331369824256.0000 - mean_absolute_error: 8070281.0000 - val_loss: 288911578365952.0000 - val_mean_absolute_error: 16632474.0000\n",
      "Epoch 8/10\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 319790614839296.0000 - mean_absolute_error: 17558998.0000 - val_loss: 147732530462720.0000 - val_mean_absolute_error: 11915926.0000\n",
      "Epoch 9/10\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 56741161074688.0000 - mean_absolute_error: 6190455.0000 - val_loss: 37042868191232.0000 - val_mean_absolute_error: 5451179.0000\n",
      "Epoch 10/10\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 80548772970496.0000 - mean_absolute_error: 8485392.0000 - val_loss: 105802375364608.0000 - val_mean_absolute_error: 9942062.0000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 102220590743552.0000 - mean_absolute_error: 9835812.0000\n",
      "Mean Absolute Error on Test Data: 9835812.0\n",
      "4/4 [==============================] - 0s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Dividir datos en entrenamiento y prueba\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Construir el modelo de recomendación sin escalado\n",
    "model = Sequential()\n",
    "\n",
    "# Capas de entrada para datos numéricos\n",
    "model.add(Dense(64, activation='relu', input_shape=(len(train_data.columns) - 1,)))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "\n",
    "# Capa de salida para predicción de puntuación\n",
    "model.add(Dense(1, activation='linear'))\n",
    "\n",
    "# Compilar el modelo\n",
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mean_absolute_error'])\n",
    "\n",
    "# Entrenar el modelo\n",
    "model.fit(train_data.drop('rating', axis=1), train_data['rating'], epochs=10, batch_size=64, validation_split=0.2)\n",
    "\n",
    "# Evaluar el modelo en el conjunto de prueba\n",
    "loss, mae = model.evaluate(test_data.drop('rating', axis=1), test_data['rating'])\n",
    "print(f'Mean Absolute Error on Test Data: {mae}')\n",
    "\n",
    "# Realizar predicciones\n",
    "predictions = model.predict(test_data.drop('rating', axis=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "5/5 [==============================] - 2s 111ms/step - loss: 427476819902464.0000 - mean_absolute_error: 18187646.0000 - val_loss: 21194115907584.0000 - val_mean_absolute_error: 4578993.0000\n",
      "Epoch 2/10\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 142278576308224.0000 - mean_absolute_error: 10989798.0000 - val_loss: 53022264655872.0000 - val_mean_absolute_error: 7242429.0000\n",
      "Epoch 3/10\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 70909037641728.0000 - mean_absolute_error: 7315640.0000 - val_loss: 85557308817408.0000 - val_mean_absolute_error: 9200024.0000\n",
      "Epoch 4/10\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 39175181041664.0000 - mean_absolute_error: 5405999.0000 - val_loss: 69832032649216.0000 - val_mean_absolute_error: 8311562.5000\n",
      "Epoch 5/10\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 29551293890560.0000 - mean_absolute_error: 4569253.0000 - val_loss: 32326612418560.0000 - val_mean_absolute_error: 5655123.0000\n",
      "Epoch 6/10\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 20442127532032.0000 - mean_absolute_error: 3963488.7500 - val_loss: 9325470810112.0000 - val_mean_absolute_error: 3037284.5000\n",
      "Epoch 7/10\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 12557929676800.0000 - mean_absolute_error: 3290863.5000 - val_loss: 1378055880704.0000 - val_mean_absolute_error: 1167641.6250\n",
      "Epoch 8/10\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 6971319123968.0000 - mean_absolute_error: 2482053.5000 - val_loss: 1994452992.0000 - val_mean_absolute_error: 44226.6328\n",
      "Epoch 9/10\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 3925650964480.0000 - mean_absolute_error: 1734961.3750 - val_loss: 168655994880.0000 - val_mean_absolute_error: 408399.2500\n",
      "Epoch 10/10\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 2323324338176.0000 - mean_absolute_error: 1384372.2500 - val_loss: 278770253824.0000 - val_mean_absolute_error: 525191.3750\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 280458559488.0000 - mean_absolute_error: 527316.6250\n",
      "Mean Absolute Error on Test Data: 527316.625\n",
      "4/4 [==============================] - 0s 4ms/step\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, Flatten, Concatenate\n",
    "\n",
    "\n",
    "# Dividir datos en entrenamiento y prueba\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalizar datos\n",
    "# scaler = StandardScaler()\n",
    "# train_data_scaled = scaler.fit_transform(train_data[['userId', 'movieId', 'timestamp_scr', 'tag_encoded']])\n",
    "# test_data_scaled = scaler.transform(test_data[['userId', 'movieId', 'timestamp_scr', 'tag_encoded']])\n",
    "\n",
    "train_data_scaled = train_data[['userId', 'movieId', 'timestamp_scr', 'tag_encoded']]\n",
    "test_data_scaled = test_data[['userId', 'movieId', 'timestamp_scr', 'tag_encoded']]\n",
    "\n",
    "# Construir el modelo de recomendación\n",
    "model = Sequential()\n",
    "\n",
    "# Capas de entrada para datos numéricos\n",
    "model.add(Dense(64, activation='relu', input_shape=(train_data_scaled.shape[1],)))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "\n",
    "# Capa de salida para predicción de puntuación\n",
    "model.add(Dense(1, activation='linear'))\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------------------------------------#\n",
    "\n",
    "# Compilar el modelo\n",
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mean_absolute_error'])\n",
    "\n",
    "# Entrenar el modelo\n",
    "model.fit(train_data_scaled, train_data['rating'], epochs=10, batch_size=64, validation_split=0.2)\n",
    "\n",
    "# Evaluar el modelo en el conjunto de prueba\n",
    "loss, mae = model.evaluate(test_data_scaled, test_data['rating'])\n",
    "print(f'Mean Absolute Error on Test Data: {mae}')\n",
    "\n",
    "# Realizar predicciones\n",
    "predictions = model.predict(test_data_scaled)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODELO CON TORCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 13.8555\n",
      "Epoch [20/100], Loss: 9.8629\n",
      "Epoch [30/100], Loss: 6.4519\n",
      "Epoch [40/100], Loss: 3.7603\n",
      "Epoch [50/100], Loss: 2.0205\n",
      "Epoch [60/100], Loss: 1.1443\n",
      "Epoch [70/100], Loss: 0.7233\n",
      "Epoch [80/100], Loss: 0.4809\n",
      "Epoch [90/100], Loss: 0.3314\n",
      "Epoch [100/100], Loss: 0.2346\n",
      "REAL RATINFG: tensor([4.0000, 5.0000, 1.5000, 4.5000, 3.0000, 4.5000, 4.0000, 4.5000, 5.0000,\n",
      "        3.5000, 4.5000, 3.5000, 5.0000, 5.0000, 4.5000, 4.0000, 3.0000, 5.0000,\n",
      "        4.0000, 4.0000, 2.0000, 4.5000, 3.0000, 4.5000, 5.0000, 4.5000, 4.5000,\n",
      "        4.5000, 5.0000, 4.0000, 5.0000, 4.0000, 4.5000, 3.0000, 1.0000, 4.5000,\n",
      "        3.5000, 2.5000, 5.0000, 5.0000, 4.0000, 1.0000, 4.5000, 4.0000, 3.0000,\n",
      "        3.0000, 1.5000, 4.0000, 4.0000, 4.5000, 4.0000, 4.5000, 4.0000, 4.5000,\n",
      "        4.0000, 4.5000, 3.5000, 4.0000, 1.0000, 3.5000, 4.0000, 3.5000, 3.0000,\n",
      "        4.0000, 2.0000, 5.0000, 3.5000, 2.0000, 5.0000, 4.0000, 4.0000, 4.5000,\n",
      "        2.0000, 3.5000, 4.5000, 3.5000, 3.5000, 3.5000, 5.0000, 3.5000, 3.5000,\n",
      "        5.0000, 1.5000, 4.5000, 4.0000, 3.5000, 5.0000, 5.0000, 4.5000, 2.5000,\n",
      "        3.5000, 2.5000, 4.5000, 2.5000, 1.5000, 4.5000, 2.5000, 3.5000, 3.0000,\n",
      "        5.0000]) ------------ PREDICT RATING tensor([4.5273, 2.0062, 2.2582, 2.0384, 3.5198, 3.7454, 3.9648, 4.7071, 3.2087,\n",
      "        3.6539, 4.4406, 1.5812, 4.4135, 2.7828, 3.1208, 3.5177, 3.0461, 2.9464,\n",
      "        3.3566, 2.2425, 4.1320, 3.4101, 4.2141, 4.2637, 4.1253, 3.9328, 4.4090,\n",
      "        4.1770, 3.7222, 2.0329, 3.4478, 3.1715, 2.1231, 4.1006, 2.6590, 2.7860,\n",
      "        3.5771, 3.3887, 3.7664, 3.9347, 4.0900, 3.5348, 2.7129, 4.3135, 3.7103,\n",
      "        3.6398, 3.5232, 3.5910, 2.2020, 3.3869, 4.0673, 1.9791, 2.5138, 1.8489,\n",
      "        5.1558, 3.5905, 2.7698, 3.0183, 3.7192, 2.6162, 2.8783, 4.0824, 1.6727,\n",
      "        4.3161, 3.3035, 2.8768, 2.9532, 2.5991, 2.9140, 4.5110, 2.8562, 4.0809,\n",
      "        2.4646, 2.6370, 3.0749, 3.5474, 3.8568, 3.4976, 3.3512, 2.3339, 3.7227,\n",
      "        3.7798, 3.0602, 2.7532, 1.2753, 4.1466, 3.1607, 4.3175, 2.9382, 5.0326,\n",
      "        3.8454, 2.8248, 3.4038, 2.4213, 2.4172, 2.9014, 4.4952, 3.6443, 2.8593,\n",
      "        2.4678])\n",
      "Test Loss: 1.8511\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Cargar datos (por ejemplo, desde un archivo CSV)\n",
    "# Suponiendo que tienes un DataFrame llamado 'data' con columnas 'userId', 'movieId' y 'rating'\n",
    "# ...\n",
    "\n",
    "# Preprocesamiento de datos\n",
    "num_movies = data['movieId'].nunique()\n",
    "\n",
    "# Define el tamaño máximo del índice + 1 (porque los índices comienzan desde 0)\n",
    "input_dim = max(max(data['userId']), max(data['movieId'])) + 1\n",
    "num_movies = input_dim\n",
    "\n",
    "# Crear embeddings para usuarios y películas\n",
    "user_embedding = nn.Embedding(input_dim, 50)\n",
    "movie_embedding = nn.Embedding(input_dim, 50)\n",
    "\n",
    "# Crear conjuntos de entrenamiento y prueba\n",
    "train, test = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Preparar datos para DataLoader\n",
    "train_users = torch.LongTensor(train['userId'].values)\n",
    "train_movies = torch.LongTensor(train['movieId'].values)\n",
    "train_ratings = torch.FloatTensor(train['rating'].values)\n",
    "\n",
    "test_users = torch.LongTensor(test['userId'].values)\n",
    "test_movies = torch.LongTensor(test['movieId'].values)\n",
    "test_ratings = torch.FloatTensor(test['rating'].values)\n",
    "\n",
    "train_dataset = TensorDataset(train_users, train_movies, train_ratings)\n",
    "test_dataset = TensorDataset(test_users, test_movies, test_ratings)\n",
    "\n",
    "# Crear DataLoader\n",
    "batch_size = 512\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# Definir el modelo de recomendación\n",
    "class Recommender(nn.Module):\n",
    "    def __init__(self, num_usr, num_movs, embedding_size=50, hidden_size=128):\n",
    "        super(Recommender, self).__init__()\n",
    "        self.user_embedding = nn.Embedding(num_usr, embedding_size)\n",
    "        self.movie_embedding = nn.Embedding(num_movs, embedding_size)\n",
    "        self.fc1 = nn.Linear(embedding_size * 2, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, user, movie):\n",
    "        user_embed = self.user_embedding(user)\n",
    "        movie_embed = self.movie_embedding(movie)\n",
    "        x = torch.cat([user_embed, movie_embed], dim=1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Instanciar el modelo, la función de pérdida y el optimizador\n",
    "model = Recommender(input_dim, num_movies)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Entrenar el modelo\n",
    "num_epochs = 100\n",
    "\n",
    "print_interval = int(num_epochs / 10) \n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for user, movie, rating in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(user, movie).view(-1)\n",
    "        loss = criterion(output, rating)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * user.size(0)\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    # Imprimir progreso cada 10% de las épocas\n",
    "    if (epoch + 1) % print_interval == 0 or epoch == num_epochs - 1:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {train_loss:.4f}')\n",
    "    # print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {train_loss:.4f}')\n",
    "\n",
    "# Evaluar el modelo en el conjunto de prueba\n",
    "model.eval()\n",
    "test_loss = 0.0\n",
    "with torch.no_grad():\n",
    "    for user, movie, rating in test_loader:\n",
    "        output = model(user, movie).view(-1)\n",
    "        print(f'REAL RATINFG: {rating} ------------ PREDICT RATING {output}')\n",
    "        loss = criterion(output, rating)\n",
    "        test_loss += loss.item() * user.size(0)\n",
    "test_loss /= len(test_loader.dataset)\n",
    "print(f'Test Loss: {test_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cargar datos (por ejemplo, desde un archivo CSV)\n",
    "# # Suponiendo que tienes un DataFrame llamado 'data' con columnas 'userId', 'movieId' y 'rating'\n",
    "# # ...\n",
    "\n",
    "# # Preprocesamiento de datos\n",
    "# # Define el tamaño máximo del índice + 1 (porque los índices comienzan desde 0)\n",
    "# input_dim = max(max(data['userId']), max(data['movieId'])) + 1\n",
    "\n",
    "# # Define el tamaño de la capa de embeddings (puedes ajustar este valor según sea necesario)\n",
    "# embedding_size = 50\n",
    "\n",
    "# # Crear conjuntos de entrenamiento y prueba\n",
    "# train, test = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Definir la arquitectura del modelo de recomendación\n",
    "# user_input = Input(shape=(1,))\n",
    "# movie_input = Input(shape=(1,))\n",
    "\n",
    "# user_embedding = Embedding(input_dim=input_dim, output_dim=embedding_size)(user_input)\n",
    "# movie_embedding = Embedding(input_dim=input_dim, output_dim=embedding_size)(movie_input)\n",
    "\n",
    "# user_flat = Flatten()(user_embedding)\n",
    "# movie_flat = Flatten()(movie_embedding)\n",
    "\n",
    "# concatenated = Concatenate()([user_flat, movie_flat])\n",
    "\n",
    "# dense_1 = Dense(128, activation='relu')(concatenated)\n",
    "# output = Dense(1)(dense_1)\n",
    "\n",
    "# # Crear el modelo\n",
    "# model = Model(inputs=[user_input, movie_input], outputs=output)\n",
    "\n",
    "# # Compilar el modelo\n",
    "# model.compile(loss='mean_squared_error', optimizer=Adam(lr=0.01))\n",
    "\n",
    "# # Entrenar el modelo\n",
    "# model.fit([train['userId'], train['movieId']], train['rating'], batch_size=512, epochs=100, validation_split=0.2, verbose=0)\n",
    "\n",
    "# # Evaluar el modelo en el conjunto de prueba\n",
    "# loss = model.evaluate([test['userId'], test['movieId']], test['rating'])\n",
    "# print(f'Loss en el conjunto de prueba: {loss}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
