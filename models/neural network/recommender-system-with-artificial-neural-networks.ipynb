{"cells":[{"cell_type":"markdown","metadata":{},"source":["https://www.kaggle.com/code/hakanerdem/recommender-system-with-artificial-neural-networks"]},{"cell_type":"markdown","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2021-06-21T13:46:39.940464Z","iopub.status.busy":"2021-06-21T13:46:39.939987Z","iopub.status.idle":"2021-06-21T13:46:42.466352Z","shell.execute_reply":"2021-06-21T13:46:42.465193Z","shell.execute_reply.started":"2021-06-21T13:46:39.940393Z"}},"source":["# Intro"]},{"cell_type":"markdown","metadata":{},"source":["Recommender systems can be used to evaluate cross selling opportunities on the domain of retail marketing. Further reading about the key concepts of \"recommender systems\",  \"cross selling\", \"collaborative filtering\" and \"deep learning\" and their applications, one can look related papers such as:\n","\n","1. Kamakura, W. A., Wedel, M., de Rosa, F., Mazzon, J. A. (2003), Cross-selling through database marketing: A mixed data factor analyzer for data augmentation and prediction. International Journal of Research in Marketing, 20, 45–65.\n","\n","2. Knott, A., Hayes, A., & Neslin, S. A. (2002), Next-product-to-buy models for cross-selling applications. Journal of Interactive Marketing, 16(3), 59–75.\n","\n","3. Thuring F., Nielsen J.P., Guillén M., Bolancé C.,(2012), Selecting prospects for cross-selling ﬁnancial products using multivariate credibility, Expert Systems with Applications 39, 8809–8816.\n","\n","4. Zhang S., Yao L., Sun A., Tay Y., (2018), Deep Learning based Recommender System: A Survey and New Perspectives. ACM Comput. Surv. 1(1), 1-35.\n","\n","5. Shi Y., Larson M., Hanjalic A., (2014), Collaborative filtering beyond the user-item matrix: A survey of the state of the art and future challenges. ACM Comput. Surv. 47(1) 45. DOI: http://dx.doi.org/10.1145/2556270\n","\n","6. Hidasi B., Karatzoglou A., (2018), Recurrent Neural Networks with Top-k Gains for Session-based Recommendations. In The 27th ACM International Conference on Information and Knowledge Management (CIKM ’18), October 22–26, 2018, Torino, Italy. ACM, New York, NY, USA, 10 pages. DOI: https://doi.org/10.1145/3269206.3271761\n","\n",".\n","."]},{"cell_type":"markdown","metadata":{},"source":["In this notebook, we, mainly aimed at prediction of customers next products to buy using implicit feedback from purchase preferences, and mainly follow [Lazy Programmer Inc.'s](https://www.udemy.com/course/recommender-systems/) -which is a udemy course about recommender systems that we strongly recommend- methodology.\n","\n","Dataset choosen, famous, Online Retail II. For detailed information please visit:\n","\n","[https://www.kaggle.com/mashlyn/online-retail-ii-uci](https://www.kaggle.com/mashlyn/online-retail-ii-uci)\n","\n","and for the original source:\n","\n","[UCI Repository](https://archive.ics.uci.edu/ml/datasets/Online+Retail+II)"]},{"cell_type":"markdown","metadata":{},"source":["# Libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-25T17:18:43.246116Z","iopub.status.busy":"2023-06-25T17:18:43.245557Z","iopub.status.idle":"2023-06-25T17:18:49.919622Z","shell.execute_reply":"2023-06-25T17:18:49.918443Z","shell.execute_reply.started":"2023-06-25T17:18:43.245955Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","\n","print(tf.version.VERSION)\n","print(keras.__version__)"]},{"cell_type":"markdown","metadata":{},"source":["# Data & Preparation"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-25T17:38:24.049406Z","iopub.status.busy":"2023-06-25T17:38:24.048953Z","iopub.status.idle":"2023-06-25T17:38:26.217144Z","shell.execute_reply":"2023-06-25T17:38:26.215976Z","shell.execute_reply.started":"2023-06-25T17:38:24.04937Z"},"trusted":true},"outputs":[],"source":["data = pd.read_csv(\"../input/online-retail-ii-uci/online_retail_II.csv\",\n","                   parse_dates=[\"InvoiceDate\"],\n","                   dtype={\"Customer ID\":\"object\"})"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-25T17:38:28.399981Z","iopub.status.busy":"2023-06-25T17:38:28.399587Z","iopub.status.idle":"2023-06-25T17:38:28.465539Z","shell.execute_reply":"2023-06-25T17:38:28.464271Z","shell.execute_reply.started":"2023-06-25T17:38:28.39994Z"},"trusted":true},"outputs":[],"source":["df = data.copy()\n","df.head()"]},{"cell_type":"markdown","metadata":{},"source":["## A brief of data cleaning\n","\n","Droping rows with missing values and irrelevant labels"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-25T17:38:31.064843Z","iopub.status.busy":"2023-06-25T17:38:31.064449Z","iopub.status.idle":"2023-06-25T17:38:31.834077Z","shell.execute_reply":"2023-06-25T17:38:31.832938Z","shell.execute_reply.started":"2023-06-25T17:38:31.064809Z"},"trusted":true},"outputs":[],"source":["df = df.dropna()\n","df = df.drop(df[df[\"Quantity\"]<0].index)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-25T17:38:34.067189Z","iopub.status.busy":"2023-06-25T17:38:34.066771Z","iopub.status.idle":"2023-06-25T17:38:34.682587Z","shell.execute_reply":"2023-06-25T17:38:34.681515Z","shell.execute_reply.started":"2023-06-25T17:38:34.067151Z"},"trusted":true},"outputs":[],"source":["df[\"StockCode\"].str.len().value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-25T17:38:36.931745Z","iopub.status.busy":"2023-06-25T17:38:36.931319Z","iopub.status.idle":"2023-06-25T17:38:39.081328Z","shell.execute_reply":"2023-06-25T17:38:39.080034Z","shell.execute_reply.started":"2023-06-25T17:38:36.931707Z"},"trusted":true},"outputs":[],"source":["df = df.drop(df[((df[\"StockCode\"].str.len()>7) | (df[\"StockCode\"].str.len()<5))].index)\n","df[\"StockCode\"] = df[\"StockCode\"].apply(lambda x: x[:5])\n","df = df.drop(df[df[\"StockCode\"].str.contains(r\"[A-Za-z]\")].index)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-25T17:38:41.914064Z","iopub.status.busy":"2023-06-25T17:38:41.913544Z","iopub.status.idle":"2023-06-25T17:38:42.546061Z","shell.execute_reply":"2023-06-25T17:38:42.544935Z","shell.execute_reply.started":"2023-06-25T17:38:41.914015Z"},"trusted":true},"outputs":[],"source":["grouped_df = df.groupby([\"Customer ID\", \"StockCode\"], as_index=False)[\"Quantity\"].sum()\n","new_df = grouped_df.astype(float).astype(int).rename(columns={\"Customer ID\": \"user_id\", \"StockCode\": \"item_id\", \"Quantity\": \"purchase\"})"]},{"cell_type":"markdown","metadata":{},"source":["By following cells, we try to generate customers' purchase sequences of distinct products. "]},{"cell_type":"markdown","metadata":{},"source":["We choose some hyperparameter values arbitrarily but it can be a good practice to look at some statistics like below: number of distinct products purchased.  "]},{"cell_type":"markdown","metadata":{},"source":["## Negative Sampling\n","\n","Since all instances prepared so far represent positive-only feedback, we try to supply some negative information to the model. Negative instances are chosen from products not purchased for a particular customer.\n","> sample_size=1 \n","\n","means there is 1 non-purchased product to be selected randomly."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-25T17:39:03.513564Z","iopub.status.busy":"2023-06-25T17:39:03.512962Z","iopub.status.idle":"2023-06-25T17:39:07.073042Z","shell.execute_reply":"2023-06-25T17:39:07.072204Z","shell.execute_reply.started":"2023-06-25T17:39:03.513511Z"},"trusted":true},"outputs":[],"source":["def neg(x, corp, sample_size=1):\n","    diff = np.setdiff1d(corp, list(x))\n","    ind = np.random.permutation(len(diff))\n","    return diff[ind[:int(sample_size*len(x))]]\n","\n","corp = new_df[\"item_id\"].unique()\n","ndf = new_df.groupby(\"user_id\", as_index=False).agg({\"item_id\": (lambda x: list(neg(x, corp, 1)))})\n","ndf = ndf.explode(\"item_id\")\n","ndf[\"purchase\"] = 0\n","sample_df = pd.concat([new_df, ndf], axis=0)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-25T18:06:01.521614Z","iopub.status.busy":"2023-06-25T18:06:01.521183Z","iopub.status.idle":"2023-06-25T18:06:03.866535Z","shell.execute_reply":"2023-06-25T18:06:03.865432Z","shell.execute_reply.started":"2023-06-25T18:06:01.521576Z"},"trusted":true},"outputs":[],"source":["VAL_SIZE = 0.2\n","\n","val_users = sample_df[\"user_id\"].drop_duplicates().sample(frac=VAL_SIZE)\n","\n","sample_df = sample_df.sample(len(sample_df))\n","\n","target_item = sample_df.groupby(\"user_id\").agg({\"item_id\": lambda x: list(x)[-1]})\n","target_item.rename(columns={\"item_id\":\"target_id\"}, inplace=True)\n","\n","train_item = sample_df.groupby(\"user_id\").agg({\"item_id\": lambda x: list(x)[:-1]})\n","\n","target_purchase = sample_df.groupby(\"user_id\").agg({\"purchase\": lambda x: list(x)[-1]})\n","target_purchase.rename(columns={\"purchase\":\"target\"}, inplace=True)\n","\n","train_purchase = sample_df.groupby(\"user_id\").agg({\"purchase\": lambda x: list(x)[:-1]}).explode(\"purchase\")\n","\n","train_data = pd.concat([target_item, train_item, target_purchase], axis=1).explode(\"item_id\")\n","train_data = pd.concat([train_data, train_purchase], axis=1).reset_index()\n","\n","train_set = train_data[[\"user_id\", \"item_id\", \"purchase\"]]\n","train_set = train_set.astype(int)\n","val_data = train_data[[\"user_id\", \"target_id\", \"target\"]].drop_duplicates()\n","val_set = val_data[val_data[\"user_id\"].isin(val_users)]\n","\n","print(\"total users:\", train_set[\"user_id\"].nunique())\n","print(\"validation users:\", val_set[\"user_id\"].nunique())"]},{"cell_type":"markdown","metadata":{},"source":["## Encoding\n","\n","As a last step we try to encode user and product features. Method taken from [keras.io](https://keras.io/examples/structured_data/collaborative_filtering_movielens/) examples. We take the data as train & validation, but the better practice is holding out some samples in advance as test data.    "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-25T18:06:09.087256Z","iopub.status.busy":"2023-06-25T18:06:09.086868Z","iopub.status.idle":"2023-06-25T18:06:10.265398Z","shell.execute_reply":"2023-06-25T18:06:10.26428Z","shell.execute_reply.started":"2023-06-25T18:06:09.087223Z"},"trusted":true},"outputs":[],"source":["user_ids = train_set[\"user_id\"].unique().tolist()\n","user2user_encoded = {x: i for i, x in enumerate(user_ids)}\n","user_encoded2user = {i: x for i, x in enumerate(user_ids)}\n","item_ids = sample_df[\"item_id\"].unique().tolist()\n","item2item_encoded = {x: i for i, x in enumerate(item_ids)}\n","item_encoded2item = {i: x for i, x in enumerate(item_ids)}\n","train_set[\"user_id\"] = train_set[\"user_id\"].map(user2user_encoded)\n","train_set[\"item_id\"] = train_set[\"item_id\"].map(item2item_encoded)\n","val_set[\"user_id\"] = val_set[\"user_id\"].map(user2user_encoded)\n","val_set[\"target_id\"] = val_set[\"target_id\"].map(item2item_encoded)\n","\n","train_set[\"purchase\"] = train_set[\"purchase\"].apply(lambda x: 1 if x>0 else 0)\n","val_set[\"target\"] = val_set[\"target\"].apply(lambda x: 1 if x>0 else 0)"]},{"cell_type":"markdown","metadata":{},"source":["# Neural Collaborative Filtering"]},{"cell_type":"markdown","metadata":{},"source":["Neural Collaborative Filtering (NCF) is one of the recommendation system frameworks, based on neural networks, proposed by He, et. al. (2017). According to them a neural network can develop a model by learning item user interactions as a key factor of a collaboritive filtering from implicit feedback. Python code mostly developed thanks to beautiful notebooks like:\n","\n","[fuzzywizard](https://www.kaggle.com/fuzzywizard/rec-sys-collaborative-filtering-dl-techniques#4-Matrix-Factorization-using-Deep-Learning-(Keras))\n","\n","[rajmehra03](https://www.kaggle.com/rajmehra03/cf-based-recsys-by-low-rank-matrix-factorization)\n","\n","Please see for detailed information about NCF:\n","\n","He, X., Liao, L., Zhang, H., Nie, L., Hu, X.,Chua, T.,  (2017), Neural Collaborative Filtering. WWW'17: Proceedings of the 26th International Conference on World Wide Web 173–182 DOI: http://dx.doi.org/10.1145/3038912.3052569"]},{"cell_type":"markdown","metadata":{},"source":["## Generalized Matrix Factorization"]},{"cell_type":"markdown","metadata":{},"source":["In the first part, we try to implement a collaborative filtering system using embedding layers for user-item instances. Python code mostly adapted from the notebooks of:   \n","\n","* [colinmorris-1](https://www.kaggle.com/colinmorris/embedding-layers)\n","* [colinmorris-2](https://www.kaggle.com/colinmorris/matrix-factorization)\n","* [rajmehra03-1](https://www.kaggle.com/rajmehra03/a-detailed-explanation-of-keras-embedding-layer)\n","* [rajmehra03-2](https://www.kaggle.com/rajmehra03/cf-based-recsys-by-low-rank-matrix-factorization)\n","* [rounakbanik](https://www.kaggle.com/rounakbanik/movie-recommender-systems)\n","* [keras.io examples](https://keras.io/examples/structured_data/collaborative_filtering_movielens/)\n","\n","and we try to evaluate the system based on the instructions from: \n","\n","[jamesloy](https://www.kaggle.com/jamesloy/deep-learning-based-recommender-systems)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-25T18:39:31.179033Z","iopub.status.busy":"2023-06-25T18:39:31.178582Z","iopub.status.idle":"2023-06-25T18:39:31.291254Z","shell.execute_reply":"2023-06-25T18:39:31.290072Z","shell.execute_reply.started":"2023-06-25T18:39:31.178994Z"},"trusted":true},"outputs":[],"source":["num_users = train_set[\"user_id\"].nunique()\n","num_items = train_set[\"item_id\"].nunique()\n","embedding_size = 64\n","\n","user_input = layers.Input(shape=(1,), name=\"user_id\", dtype=tf.int32)\n","item_input = layers.Input(shape=(1,), name=\"item_id\", dtype=tf.int32)\n"," \n","user_embedding = layers.Embedding(num_users,\n","                                  embedding_size,\n","                                  name=\"user_emb\")(user_input)\n","user_bias = layers.Embedding(num_users, 1, name=\"user_bias\")(user_input)\n"," \n","item_embedding = layers.Embedding(num_items,\n","                                  embedding_size,\n","                                  name=\"item_emb\")(item_input)\n","item_bias = layers.Embedding(num_items, 1, name=\"item_bias\")(item_input)\n"," \n","user_vector = layers.Flatten()(user_embedding)\n","item_vector = layers.Flatten()(item_embedding)\n"," \n","dot_user_item = layers.Dot(name=\"dot\", axes=1)([user_vector, item_vector])\n","output = layers.Add(name=\"add\")([dot_user_item, user_bias, item_bias])\n","output = layers.Flatten(name=\"flat\")(output)\n"," \n","model_X = keras.Model([user_input, item_input], output, name=\"model_x\")\n"," \n","model_X.compile(loss=tf.keras.losses.MeanSquaredError(),\n","                optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n","                metrics=[\"mae\"])\n"," \n","model_X.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-25T18:34:26.688529Z","iopub.status.busy":"2023-06-25T18:34:26.688124Z","iopub.status.idle":"2023-06-25T18:38:43.93713Z","shell.execute_reply":"2023-06-25T18:38:43.93614Z","shell.execute_reply.started":"2023-06-25T18:34:26.688494Z"},"trusted":true},"outputs":[],"source":["es = keras.callbacks.EarlyStopping(monitor=\"val_loss\",\n","                                   mode=\"min\",\n","                                   verbose=1,\n","                                   patience=3)\n","\n","history = model_X.fit([train_set[\"user_id\"].values, train_set[\"item_id\"].values],\n","                      train_set[\"purchase\"].values,\n","                      batch_size=256,\n","                      epochs=200,\n","                      verbose=1,\n","                      validation_data=([val_set[\"user_id\"].values, val_set[\"target_id\"].values], val_set[\"target\"].values),\n","                      callbacks=[es])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-25T18:39:01.291538Z","iopub.status.busy":"2023-06-25T18:39:01.290941Z","iopub.status.idle":"2023-06-25T18:39:01.496234Z","shell.execute_reply":"2023-06-25T18:39:01.495136Z","shell.execute_reply.started":"2023-06-25T18:39:01.291499Z"},"trusted":true},"outputs":[],"source":["plt.plot(history.history[\"loss\"])\n","plt.plot(history.history[\"val_loss\"])\n","plt.title(\"embedding loss\")\n","plt.ylabel(\"loss\")\n","plt.xlabel(\"epoch\")\n","plt.legend([\"train\", \"val\"], loc=\"best\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["## Multi-Layer Perceptron"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-25T18:58:43.680136Z","iopub.status.busy":"2023-06-25T18:58:43.679675Z","iopub.status.idle":"2023-06-25T18:58:43.794157Z","shell.execute_reply":"2023-06-25T18:58:43.793127Z","shell.execute_reply.started":"2023-06-25T18:58:43.680097Z"},"trusted":true},"outputs":[],"source":["hidden_units = [128, 64]\n"," \n","user_input = layers.Input(shape=(1,), name=\"user_id\", dtype=tf.int32)\n","item_input = layers.Input(shape=(1,), name=\"item_id\", dtype=tf.int32)\n","user_embedding = layers.Embedding(num_users,\n","                                  embedding_size,\n","                                  name=\"user_emb\")(user_input)\n","item_embedding = layers.Embedding(num_items,\n","                                  embedding_size,\n","                                  name=\"item_emb\")(item_input)\n","\n","concatenated = layers.Concatenate(name=\"concat\")([user_embedding, item_embedding])\n","out = layers.Flatten(name=\"flat\")(concatenated)\n"," \n","for n_hidden in hidden_units:\n","    out = layers.Dense(n_hidden,\n","                       activation=\"relu\")(out)\n","    out = layers.Dropout(0.4)(out)\n","    out = layers.BatchNormalization()(out)\n","\n","out = layers.Dense(1, activation=\"sigmoid\", name=\"prediction\")(out)\n"," \n","model_Y = keras.Model(inputs = [user_input, item_input],\n","                      outputs = out, name=\"model_y\")\n"," \n","model_Y.compile(loss=keras.losses.MeanSquaredError(),\n","                optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n","                metrics=[\"mae\"])\n"," \n","model_Y.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-25T18:40:22.428796Z","iopub.status.busy":"2023-06-25T18:40:22.428368Z","iopub.status.idle":"2023-06-25T18:43:41.457835Z","shell.execute_reply":"2023-06-25T18:43:41.456958Z","shell.execute_reply.started":"2023-06-25T18:40:22.428758Z"},"trusted":true},"outputs":[],"source":["es = keras.callbacks.EarlyStopping(monitor=\"val_loss\",\n","                                   mode=\"min\",\n","                                   verbose=1,\n","                                   patience=3)\n","\n","history = model_Y.fit([train_set[\"user_id\"].values, train_set[\"item_id\"].values],\n","                      train_set[\"purchase\"].values,\n","                      batch_size=256,\n","                      epochs=200,\n","                      verbose=1,\n","                      validation_data=([val_set[\"user_id\"].values, val_set[\"target_id\"].values], val_set[\"target\"].values),\n","                      callbacks=[es])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-25T18:44:55.89376Z","iopub.status.busy":"2023-06-25T18:44:55.89333Z","iopub.status.idle":"2023-06-25T18:44:56.098571Z","shell.execute_reply":"2023-06-25T18:44:56.09747Z","shell.execute_reply.started":"2023-06-25T18:44:55.893725Z"},"trusted":true},"outputs":[],"source":["plt.plot(history.history[\"loss\"])\n","plt.plot(history.history[\"val_loss\"])\n","plt.title(\"mlp loss\")\n","plt.ylabel(\"loss\")\n","plt.xlabel(\"epoch\")\n","plt.legend([\"train\", \"val\"], loc=\"best\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["## Neural Matrix Factorization"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-25T19:00:53.997019Z","iopub.status.busy":"2023-06-25T19:00:53.996361Z","iopub.status.idle":"2023-06-25T19:00:54.15409Z","shell.execute_reply":"2023-06-25T19:00:54.153277Z","shell.execute_reply.started":"2023-06-25T19:00:53.996958Z"},"trusted":true},"outputs":[],"source":["hidden_units = [128, 64]\n","\n","user_input = layers.Input(shape=(1,), name=\"user_id\", dtype=tf.int32)\n","item_input = layers.Input(shape=(1,), name=\"item_id\", dtype=tf.int32)\n","user_embedding = layers.Embedding(num_users,\n","                                  embedding_size,\n","                                  name=\"user_emb\")(user_input)\n","user_bias = layers.Embedding(num_users, 1, name=\"user_bias\")(user_input)\n"," \n","item_embedding = layers.Embedding(num_items,\n","                                  embedding_size,\n","                                  name=\"item_emb\")(item_input)\n","item_bias = layers.Embedding(num_items, 1, name=\"item_bias\")(item_input)\n","\n","user_vector = layers.Flatten()(user_embedding)\n","item_vector = layers.Flatten()(item_embedding)\n","\n","dot_user_item = layers.Dot(name=\"dot\", axes=1)([user_vector, item_vector])\n","X = layers.Add(name=\"add\")([dot_user_item, user_bias, item_bias])\n","X = layers.Flatten(name=\"flat1\")(X)\n","\n","concatenated = layers.Concatenate(name=\"concat\")([user_embedding, item_embedding])\n","Y = layers.Flatten(name=\"flat2\")(concatenated)\n"," \n","for n_hidden in hidden_units:\n","    Y = layers.Dense(n_hidden,\n","                     activation=\"relu\")(Y)\n","    Y = layers.Dropout(0.4)(Y)\n","    Y = layers.BatchNormalization()(Y)\n","\n","Y = layers.Dense(1, activation=\"sigmoid\", name=\"prediction\")(Y)\n","\n","Z = layers.Add(name=\"final\")([X, Y])\n","\n","model_Z = keras.Model(inputs = [user_input, item_input],\n","                      outputs = Z, name=\"model_z\")\n"," \n","model_Z.compile(loss=keras.losses.MeanSquaredError(),\n","                optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n","                metrics=[\"mae\"])\n"," \n","model_Z.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-25T19:01:31.068815Z","iopub.status.busy":"2023-06-25T19:01:31.06841Z","iopub.status.idle":"2023-06-25T19:04:30.301081Z","shell.execute_reply":"2023-06-25T19:04:30.299934Z","shell.execute_reply.started":"2023-06-25T19:01:31.068782Z"},"trusted":true},"outputs":[],"source":["es = keras.callbacks.EarlyStopping(monitor=\"val_loss\",\n","                                   mode=\"min\",\n","                                   verbose=1,\n","                                   patience=3)\n","\n","history = model_Z.fit([train_set[\"user_id\"].values, train_set[\"item_id\"].values],\n","                      train_set[\"purchase\"].values,\n","                      batch_size=256,\n","                      epochs=200,\n","                      verbose=1,\n","                      validation_data=([val_set[\"user_id\"].values, val_set[\"target_id\"].values], val_set[\"target\"].values),\n","                      callbacks=[es])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-08-17T18:10:43.292259Z","iopub.status.busy":"2021-08-17T18:10:43.291895Z","iopub.status.idle":"2021-08-17T18:10:43.449413Z","shell.execute_reply":"2021-08-17T18:10:43.448511Z","shell.execute_reply.started":"2021-08-17T18:10:43.292227Z"},"trusted":true},"outputs":[],"source":["plt.plot(history.history[\"loss\"])\n","plt.plot(history.history[\"val_loss\"])\n","plt.title(\"ncf loss\")\n","plt.ylabel(\"loss\")\n","plt.xlabel(\"epoch\")\n","plt.legend([\"train\", \"val\"], loc=\"best\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["# Model Evaluation\n","\n","We try to measure the model performance by providing candidate products to the model and evaluating the outputs. Candidate products are merged with 49 products selected from non-purchased products and a target product which respresented in output_sequence variable. If target product occures in the top k of the model outputs, we count this event as a hit.\n","\n","On the other hand; Hidasi and Karatzoglou (2018) define \"recall@k\" as an evaluatinon metric as \"the proportion of cases having the desired item amongst the top-k items in all test cases.\" Moreover, one another evaluation metric is \"MRR@k\", which is the average of reciprocal ranks of the target items. The reciprocal rank is set to zero if the rank is above k."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-08-17T19:02:05.665241Z","iopub.status.busy":"2021-08-17T19:02:05.664818Z","iopub.status.idle":"2021-08-17T19:02:05.675304Z","shell.execute_reply":"2021-08-17T19:02:05.674301Z","shell.execute_reply.started":"2021-08-17T19:02:05.665199Z"},"trusted":true},"outputs":[],"source":["# def get_metrics(model, k, size=1000):  \n","#     hit = 0\n","#     mrr = 0\n","#     counter = size\n","#     for s in range(counter):\n","#         cust_id = sample_df[\"CustomerID\"].unique()[s]\n","#         cust_encoder = cust2cust_encoded.get(cust_id)\n","#         purchased = frequent_df[(frequent_df.index==cust_id) & (frequent_df[\"purchase\"]==1)]\n","#         candidates = frequent_df[~frequent_df[\"input_sequence\"].isin(purchased[\"input_sequence\"].values)][\"input_sequence\"][:49]\n","#         candidates = set(candidates).intersection(set(prod2prod_encoded.keys()))\n","#         candidates = candidates.union(set(frequent_df[frequent_df.index==cust_id][\"output_sequence\"].values[0]))\n","#         candidates = [[prod2prod_encoded.get(x)] for x in list(candidates)]\n","#         ids = np.stack([[cust_encoder]]*len(candidates))\n","#         y_pred = model.predict([ids, np.array(list(candidates), dtype=\"int32\")]).flatten()\n","#         t = frequent_df.loc[(frequent_df.index==cust_id), \"output_sequence\"].values[0][0]\n","#         recommend = []\n","#         rr = 0\n","#         for i in range(k):\n","#             p = prod_encoded2prod.get(candidates[y_pred.argsort()[-(i+1)]][0])\n","#             recommend.append(p)\n","#             if (p==t):\n","#                 rr = 1/(i+1)\n","#                 hit = hit + 1\n","#         mrr = mrr + rr\n","        \n","#     return (hit/counter), (mrr/counter)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-08-17T19:02:10.870712Z","iopub.status.busy":"2021-08-17T19:02:10.870306Z","iopub.status.idle":"2021-08-17T19:08:29.428925Z","shell.execute_reply":"2021-08-17T19:08:29.427708Z","shell.execute_reply.started":"2021-08-17T19:02:10.870671Z"},"trusted":true},"outputs":[],"source":["# recallx, mrrx = get_metrics(model_X, 10)\n","# print(\"Recall@: \", recallx)\n","# print(\"MRR@: \", mrrx)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-08-17T19:09:10.731518Z","iopub.status.busy":"2021-08-17T19:09:10.731149Z","iopub.status.idle":"2021-08-17T19:15:28.807521Z","shell.execute_reply":"2021-08-17T19:15:28.80645Z","shell.execute_reply.started":"2021-08-17T19:09:10.731484Z"},"trusted":true},"outputs":[],"source":["# recally, mrry = get_metrics(model_Y, 10)\n","# print(\"Recall@: \", recally)\n","# print(\"MRR@: \", mrry)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-08-17T19:17:06.632064Z","iopub.status.busy":"2021-08-17T19:17:06.63168Z","iopub.status.idle":"2021-08-17T19:23:21.548399Z","shell.execute_reply":"2021-08-17T19:23:21.547358Z","shell.execute_reply.started":"2021-08-17T19:17:06.632032Z"},"trusted":true},"outputs":[],"source":["# recallz, mrrz = get_metrics(model_Z, 10)\n","# print(\"Recall@: \", recallz)\n","# print(\"MRR@: \", mrrz)"]},{"cell_type":"markdown","metadata":{},"source":["# Critiques\n","\n","Please criticise this study and faulty issues other than hyperparameter tuning. Any comment is more precious than upvotes for this fresh notebook. To compare metrics please see: https://medium.com/decathlondevelopers/building-a-rnn-recommendation-engine-with-tensorflow-505644aa9ff3. They developed a model for more than 10,000 different products. \n","\n","Some hyperparameters which should be tuned.\n","\n","* Number and unit numbers of *hidden_units*\n","* *prod_embedding_size* and *user_embedding_size*\n","* regularizers, learning rate, activation functions, batch size, dropout rates\n","* *n_frequency* number of frequent products\n","* *sample_size* number of negative samples corresponding to a positive sample\n","\n","Some topics which are ambiguous:\n","\n","* Can prediction performance be upgraded for this model? \n","* Is there a need for negative sampling? Is there a room for improvement by adjusting negative sample size?\n","* Are there more suitable or effective techniques to measure the performance of the model?\n","* Can execution time be shortened?\n","* Any other effective ways to predict next-product-to-buy using deep learning?\n",".\n",".\n","\n","Sorry for language...\n","Thanks in advance"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":430934,"sourceId":821251,"sourceType":"datasetVersion"}],"dockerImageVersionId":30096,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":4}
