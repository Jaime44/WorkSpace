{"cells":[{"cell_type":"markdown","metadata":{},"source":["https://www.kaggle.com/code/fuzzywizard/rec-sys-collaborative-filtering-dl-techniques/notebook"]},{"cell_type":"markdown","metadata":{},"source":["# Rec Sys -> Collaborative Filtering & DL Techniques\n","* **You can also view and contribute to the notebook below.**\n","* **Github - [Recommender-Systems-with-Collaborative-Filtering-and-Deep-Learning-Techniques](https://github.com/Chinmayrane16/Recommender-Systems-with-Collaborative-Filtering-and-Deep-Learning-Techniques)**\n","* **Do Star/Upvote if you like it :)**"]},{"cell_type":"markdown","metadata":{"_uuid":"6c0f989904faf30bd6df4f6982a3fb3475278b77"},"source":["# What are Recommendation Systems?"]},{"cell_type":"markdown","metadata":{"_uuid":"ce89039a986c7744a4053976634d737eacc51d58"},"source":["Every day we view and purchase products on Amazon, browse Netflix, stream music on Spotify. Have you ever wondered how accurately they know your tastes and preferences and make Recommendations based on your interests ?\n","\n","The answer is they have personalized Recommendation Engines which uses advanced algorithms to recommend products to you... When you want to watch a new movie, you usually ask your friends for suggesting movies, they know your interests and accordingly they suggest you a good movie.\n","\n","So, **Recommendation Systems work the same way, they are nothing but data filtering tools that uses algorithms to recommend most relevant items to a particular user.**\n","\n","Now, let us understand Why do we need Recommendation Systems and in the later part we will look at How Recommender Systems actually work?\n","\n","![](https://i.imgur.com/DlXYzzg.jpg)"]},{"cell_type":"markdown","metadata":{"_uuid":"3bfb230af118afa9b264b5515e9e1ab31ce58f2e"},"source":["# Why do we need Recommendation Systems?"]},{"cell_type":"markdown","metadata":{"_uuid":"c4d599b1a68b9d974798d832f8e01b00e5bae20a"},"source":["Internet is a vast ocean of Information, it consists of millions of items, a really large catalogue of products. There are some Users that know what they are looking for whereas others have no idea what to look for in such a large library of resources, and thus, Recommendation Systems play a vital role.\n","\n","There are some products which may be really good but have not gained **Popularity** since they have not been advertised, so recommendation systems help such items gain popularity by bringing such items to one's notice.\n","\n","It even helps in **Ad Targeting**- Say you're looking to buy a new laptop on Internet, your recent searches are based on laptop suggestions, and soon you'll start seeing ads on websites offering discounts on laptops. So, Ad Targeting is an advertisement technique meant to deliver ads automatically by using specialized software and algorithms that place ads depending on the userâ€™s recent searches. Ad targeting was pegged to have secured 2.7 times as much revenue as non-targeted ads, as shown by a study conducted in 2009 by the Network Advertising Initiative.\n","\n","Thus, it also helps in **Increasing Revenues** of the product's Organization."]},{"cell_type":"markdown","metadata":{"_uuid":"605768a5174a2cdc24fd9c42539cdf2156c8fd3f"},"source":["# Contents\n","1. [**Exploring the Dataset**](#there_you_go_1)\n","> *  [1.1 Importing Libraries ](#there_you_go_1.1)\n","  * [1.2 Extract dataset ](#there_you_go_1.2)\n","  >> * [1.2.1 Ratings](#there_you_go_1.2.1)\n","    * [1.2.2 Movies](#there_you_go_1.2.2)\n","  * [1.3 Combining the Movies and Ratings Dataframe ](#there_you_go_1.3)\n","2. [**Visualizing the Dataset**](#there_you_go_2)\n","> * [2.1 Genres ](#there_you_go_2.1)\n","  * [2.2 Heavily Rated Movies ](#there_you_go_2.2)\n","  * [2.3 Highly rated Movies](#there_you_go_2.3)\n","  * [2.4 Mean ratings vs Total number of ratings](#there_you_go_2.4)\n","3. [**Collaborative Filtering**](#there_you_go_3)\n","> * [3.1 Create User-Item Matrix ](#there_you_go_3.1)\n"," * [3.2 Memory Based CF ](#there_you_go_3.2)\n"," >> * [3.2.1 User Based CF](#there_you_go_3.2.1)\n","     * [3.2.2 Item based CF](#there_you_go_3.2.2)\n"," * [3.3 Model Based CF ](#there_you_go_3.3)\n"," >> * [3.3.1 K-Nearest Neighbour](#there_you_go_3.3.1)\n","    * [3.3.2 Singular Value Decomposition](#there_you_go_3.3.2)\n","    * [3.3.3 Non-Negative Matrix Factorization](#there_you_go_3.3.3)\n","4. [**Matrix Factorization using Deep Learning**](#there_you_go_4)\n","> * [4.1 Splitting Data into Train and Validation Set ](#there_you_go_4.1)\n"," * [4.2 Building the Model using Embedding Layers ](#there_you_go_4.2)\n"," * [4.3 Architecture 1 ](#there_you_go_4.3)\n"," * [4.4 Visulaizing the Model Architecture](#there_you_go_4.4)\n"," * [4.5 Compiling the Model ](#there_you_go_4.5)\n"," * [4.6 Fitting the Model](#there_you_go_4.6)\n"," * [4.7 Plotting Validation Curves](#there_you_go_4.7)\n"," * [4.8 Evaluating RMSE](#there_you_go_4.8)\n"," * [4.9 Architecture 2](#there_you_go_4.9)\n"," * [4.10 Visulaizing the Model Architecture](#there_you_go_4.10)\n"," * [4.11 Compiling the Model ](#there_you_go_4.11)\n"," * [4.12 Fitting the Model](#there_you_go_4.12)\n"," * [4.13 Plotting Validation Curves](#there_you_go_4.13)\n"," * [4.14 Evaluating RMSE](#there_you_go_4.14)\n","5. [**References**](#there_you_go_5)"]},{"cell_type":"markdown","metadata":{"_uuid":"b1c20eff9bfa834c52cc14c7ff3c502198e18d2e"},"source":["<a id=\"there_you_go_1\"></a>\n","# 1) Exploring the Dataset\n","The dataset we are going to use is the MovieLens Dataset, which cotains 100k ratings of approximately 9000 movies by 700 users.\n","Let's have a look at the dataset."]},{"cell_type":"markdown","metadata":{"_uuid":"0a483be448fcd08898f87bf8136984055df69d8a"},"source":["<a id=\"there_you_go_1.1\"></a>\n","## 1.1) Importing the Libraries "]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["# Ignore warnings :\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","\n","# Handle table-like data and matrices :\n","import numpy as np\n","import pandas as pd\n","import math \n","import itertools\n","\n","# Modelling Helpers :\n","from sklearn.preprocessing import Imputer , Normalizer , scale\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import confusion_matrix\n","from sklearn.feature_selection import RFECV\n","from sklearn.model_selection import GridSearchCV , KFold , cross_val_score\n","\n","\n","\n","# Evaluation metrics :\n","\n","# Regression\n","from sklearn.metrics import mean_squared_log_error,mean_squared_error, r2_score,mean_absolute_error \n","\n","# Classification\n","from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score\n","\n","\n","# Deep Learning Libraries\n","from keras.models import Sequential, load_model\n","from keras.layers import Dense, Dropout, Flatten\n","from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n","from keras.optimizers import Adam,SGD,Adagrad,Adadelta,RMSprop\n","from keras.preprocessing.image import ImageDataGenerator\n","from keras.callbacks import ReduceLROnPlateau, LearningRateScheduler\n","from keras.utils import to_categorical\n","\n","\n","# Visualisation\n","import matplotlib as mpl\n","import matplotlib.pyplot as plt\n","import matplotlib.pylab as pylab\n","import seaborn as sns\n","import missingno as msno\n","\n","\n","# Configure visualisations\n","%matplotlib inline\n","mpl.style.use( 'ggplot' )\n","plt.style.use('fivethirtyeight')\n","sns.set(context=\"notebook\", palette=\"dark\", style = 'whitegrid' , color_codes=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"outputs":[],"source":["# Center all plots\n","from IPython.core.display import HTML\n","HTML(\"\"\"\n","<style>\n",".output_png {\n","    display: table-cell;\n","    text-align: center;\n","    vertical-align: middle;\n","}\n","</style>\n","\"\"\");\n","\n","\n","# Make Visualizations better\n","params = { \n","    'axes.labelsize': \"large\",\n","    'xtick.labelsize': 'x-large',\n","    'legend.fontsize': 20,\n","    'figure.dpi': 150,\n","    'figure.figsize': [25, 7]\n","}\n","plt.rcParams.update(params)"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"7757d7eb49a78b0b35f168d720f024ce173272b7","trusted":true},"outputs":[],"source":["import os\n","print(os.listdir('../input/'))"]},{"cell_type":"markdown","metadata":{"_uuid":"584d9c76c6aa62617e9c4e3317538f72b8edf348"},"source":["<a id=\"there_you_go_1.2\"></a>\n","## 1.2) Extract Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"82b99b1c7d3e3b31dc931858723cf5f94d4609b3","trusted":true},"outputs":[],"source":["ratings = pd.read_csv('../input/ratings.csv')\n","movies = pd.read_csv('../input/movies.csv')\n","df_r = ratings.copy()\n","df_m = movies.copy()"]},{"cell_type":"markdown","metadata":{"_uuid":"8cc4959db5ac479ea2fa78d250e1be2fab81fc4c"},"source":["<a id=\"there_you_go_1.2.1\"></a>\n","### 1.2.1) Ratings"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"4c97be375d2ecf347ea13910686cc6942c378844","scrolled":true,"trusted":true},"outputs":[],"source":["ratings.head()"]},{"cell_type":"markdown","metadata":{"_uuid":"e3ec44591a522a197809987ac97070c23da2c78b"},"source":["As you can see the \"ratings\" dataframe has 4 columns.\n","* **userId** - Every user is represented by an unique Id.\n","* **movieId** - Every movie is represented by an uniue Id.\n","* **rating** - Represents the rating given by the user to the corresponding movie.\n","* **timestamp** - The time at which the rating was recorded."]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"cbf5afdd3663c9f0a206da2244063f41f4e87384","trusted":true},"outputs":[],"source":["ratings.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"6193a5bdb14f1bd80f84ba9327619c4f7c653802","trusted":true},"outputs":[],"source":["ratings.describe()"]},{"cell_type":"markdown","metadata":{"_uuid":"d40afd33063271059ac295ac246146c41b2f6b07"},"source":["Some insights that can be drawn are:\n","\n","1. The minimum rating given to the movie is 0,5 whereas the maximum rating given to the movies is 5.0\n","2. The average rating that is the mean ratings given by the users to all the movies is 3.5\n","3. The users have userId's in the range 1 - 610\n","4. The movies have movieId's in the range 1 - 193609.( Note that 193609 is the highest movieId and not total number of movies. )\n","\n","Here, we will be dropping the timestamp attribute as we are not concerned with when the user rated a particular movie."]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"a5a04e2b5bc7625bba176707ef18194dd98afdeb","trusted":true},"outputs":[],"source":["ratings.drop(['timestamp'], axis=1, inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"25643d5a70b2dcf458d061f709aaef4a65f907a7","trusted":true},"outputs":[],"source":["ratings.head()"]},{"cell_type":"markdown","metadata":{"_uuid":"0db882f8b0ba7ebb4be2e6099a866fe5515bce37"},"source":["<a id=\"there_you_go_1.2.2\"></a>\n","### 1.2.2) Movies"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"b85ac976102e84c1070c5439e07fde2a6d7e29d4","trusted":true},"outputs":[],"source":["movies.head()"]},{"cell_type":"markdown","metadata":{"_uuid":"83d52a2ec24be3b1ed019da5a88cd2ead2a15033"},"source":["As you can see the \"movies\" dataframe has 3 columns:\n","* **movieId** - Every movie is represented by an unique Id.\n","* **title** - Movie which is represented by the corresponding movieId.\n","* **genres** - Represents category of the movie."]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"f172700ce0f457bf8d477f88c09d110984436406","trusted":true},"outputs":[],"source":["print('Shape: ', movies.shape, '\\n')\n","movies.info()"]},{"cell_type":"markdown","metadata":{"_uuid":"61294765bc05f49116a0f1ff287b48c007246327"},"source":["<a id=\"there_you_go_1.3\"></a>\n","## 1.3) Combining the Movies and Ratings DataFrame\n","Let's have a combined view on both the ratings and movies dataframe.\n","\n","And for that we need to merge on \"movieId\" attribute since it is common between both the dataframes."]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"8f3642d37de0d4e04487e65b089ed12119d69e9f","trusted":true},"outputs":[],"source":["df_combined = pd.merge(ratings, movies, on = 'movieId')"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"5c2c75b9f091959f3f36420ac2140858d00706cb","trusted":true},"outputs":[],"source":["df_combined.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"31b90ef3e7ea9cb480f55e2a07ff5e9823cd0458","trusted":true},"outputs":[],"source":["df_combined.shape"]},{"cell_type":"markdown","metadata":{"_uuid":"09928fa4f6e71d5a508fcedd6557a11ffbf7ebd2"},"source":["<a id=\"there_you_go_2\"></a>\n","# 2) Visualizations on the Dataset"]},{"cell_type":"markdown","metadata":{"_uuid":"2fe4775471fd87bfe678f65a4b915ffff6a8a8d2"},"source":["<a id=\"there_you_go_2.1\"></a>\n","## 2.1) Genres\n","Lets have a look at various genres in the Dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"a897520396787e720f8ca7198a3f7baa1517ab77","trusted":true},"outputs":[],"source":["# Create a function to find genres in the dataset\n","\n","genres = {} # create a dictionary to store different genre values\n","\n","def find_genres():\n","    for genre in movies['genres']:\n","        words = genre.split('|')\n","        for word in words:\n","            genres[word] = genres.get(word, 0) + 1\n","            \n","find_genres()"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"952c4be8b9e0bdace08883beaa399d8f230f0584","trusted":true},"outputs":[],"source":["genres"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"304e7477eabdd4326ba063b621e66432bbeffcdb","trusted":true},"outputs":[],"source":["# replace '(no genres listed)' by 'None'\n","genres['None'] = genres.pop('(no genres listed)')"]},{"cell_type":"markdown","metadata":{"_uuid":"0b90c9816f32a5d944f0e00c19ddfa209495091a"},"source":["<a id=\"there_you_go_2.1.1\"></a>\n","### 2.1.1) Genre WordCloud"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"7d252757351e8d36fd3b394fa8bd84c7f3156ed1","trusted":true},"outputs":[],"source":["from wordcloud import WordCloud\n","\n","wordcloud = WordCloud(width=400, height=200, background_color = 'black', min_font_size=7).generate_from_frequencies(genres)\n","\n","plt.imshow(wordcloud)\n","plt.axis('off')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"_uuid":"85c25f351caa1130395a776dd30a6880e5d30d45"},"source":["<a id=\"there_you_go_2.2\"></a>\n","## 2.2) Heavily Rated Movies"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"c47ad156607971607898ae65749a381061f09b02","trusted":true},"outputs":[],"source":["df_n_ratings = pd.DataFrame(df_combined.groupby('title')['rating'].mean())\n","df_n_ratings['total ratings'] = pd.DataFrame(df_combined.groupby('title')['rating'].count())\n","df_n_ratings.rename(columns = {'rating': 'mean ratings'}, inplace=True)\n","\n","df_n_ratings.sort_values('total ratings', ascending=False).head(10)"]},{"cell_type":"markdown","metadata":{"_uuid":"004505c4a7e7bc97ba9cf99bb84cda1697999706"},"source":["Most of these movies stand among the Top 50 movies in the IMDB ratings even today."]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"1ef247ab14cae32999e18bebc9e77b1573cf25f6","trusted":true},"outputs":[],"source":["plt.figure(figsize=(8,4))\n","sns.distplot(df_n_ratings['total ratings'], bins=20)\n","plt.xlabel('Total Number of Ratings')\n","plt.ylabel('Probability')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"_uuid":"8e4300398ad483f6eae5c794d39d857590edf137"},"source":["We can see that - \n","* Majority of the movies have less than 50 ratings.\n","* The number of movies having more than 100 ratings is very low.\n","\n","Refer one cell above, we can see that there are only 3 movies with 300+ ratings."]},{"cell_type":"markdown","metadata":{"_uuid":"78d81e82705c100c4a940671a0c9b46b1709c473"},"source":["<a id=\"there_you_go_2.3\"></a>\n","## 2.3) Highly Rated Movies"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"e85d5bb8cc0889b9359ce84833ca970ca3d6607e","trusted":true},"outputs":[],"source":["df_n_ratings.sort_values('mean ratings', ascending=False).head(10)"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"710829904ef7b77eea10789c5babea7e5843e027","trusted":true},"outputs":[],"source":["print('Total no of users that gave rating of 5.0 : ', len(df_n_ratings.loc[df_n_ratings['mean ratings'] == 5]), '\\n')\n","print('Total no of Individual users that gave rating of 5.0 : ', len(df_n_ratings.loc[(df_n_ratings['mean ratings'] == 5) \n","                                                                           & (df_n_ratings['total ratings'] == 1)]))"]},{"cell_type":"markdown","metadata":{"_uuid":"0b148aadede48005bb067748d7a227ff5d844f39"},"source":["1. As you can see there are over 296 users that have rated 5 stars, among which there are 289 individual raters ( only user to rate the movie 5 star ).\n","\n","2. So, this cannot be the lone factor that should be considered while recommending movies. As this factor only shows the preferences of a particular user.\n","\n","3. It would make a good recommendation system if we can use both the factors ( -> Highly Rated Movies and Heavily Rated Movies <- ) together."]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"85594914dfca2b6d1130cacb55e57d12a9f011c9","trusted":true},"outputs":[],"source":["plt.figure(figsize=(8,4))\n","sns.distplot(df_n_ratings['mean ratings'], bins=30)\n","plt.xlabel('Mean Ratings')\n","plt.ylabel('Probability')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"_uuid":"3658e4d3d80fd064f48589f529cd4594c9b87721"},"source":["<a id=\"there_you_go_2.4\"></a>\n","## 2.4) Mean Ratings vs Total Number of Ratings"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"af127ded1183c61079fa2395d181996e48904015","trusted":true},"outputs":[],"source":["sns.jointplot(x = 'mean ratings', y = 'total ratings', data = df_n_ratings )"]},{"cell_type":"markdown","metadata":{"_uuid":"f4bb3964e26346357c1a1da3b5bea316491f5379"},"source":["* Here, as you can see every Data Point represents a distinct Movie, with y-coordinate representing the total no of users which has rated that movie and x-coordinate representing the mean of all the ratings of the corresponding users.\n","* Also you can see that there is a huge Density in the region corresponding to 0-50 no of users and between mean rating 3-4 ."]},{"cell_type":"markdown","metadata":{"_uuid":"8e9adee013ed2c8e089ebc17bcb30d8a2fd45efc"},"source":["Many more Visualizations can be drawn and many different conclusions can be inferred, But here, I'm going to focus on Collaborative Filtering.\n","\n","So, let's proceed to what is Collaborative Filtering and how it is used in Recommendation Systems?"]},{"cell_type":"markdown","metadata":{"_uuid":"90c097447ca91831c3f2589158e574dbc4f22176"},"source":["<a id=\"there_you_go_3\"></a>\n","# 3) Collaborative Filtering (CF)"]},{"cell_type":"markdown","metadata":{"_uuid":"d526c06b1c8c6acb67896f5715ef663a2865ed11"},"source":["**Collaborative filtering** is the process of filtering for information or patterns using techniques involving collaboration among multiple agents, viewpoints, data sources.Basically, it is a method of making automatic predictions (filtering) about the interests of a user by collecting preferences or taste information from many users. \n","\n","There are 2 approaches to CF -->\n","\n","1) **Memory-Based CF** - It is an approach which finds similarity between users or between items to recommend similar items. Examples include Neighbourhood-based CF and Item-based/User-based top-N recommendations.\n","\n","2) **Model-Based CF** - In this approach we use different data mining, machine learning algorithms to predict users' rating of unrated items.  Examples include Singular Value Decomposition (SVD) , Principal Component Analysis (PCA) etc."]},{"cell_type":"markdown","metadata":{"_uuid":"d5d73487ddb54a1a5103a1214419e22ae42a483c"},"source":["<a id=\"there_you_go_3.1\"></a>\n","## 3.1) Create User-Item Matrix"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"c9cdb0771a1eaf51732caeea3b9dc0944a3310cc","trusted":true},"outputs":[],"source":["util_mat = df_combined.pivot_table(index = 'userId', columns = 'title', values = 'rating')\n","util_mat.head(20)"]},{"cell_type":"markdown","metadata":{"_uuid":"ee8a4b5af65524dd3885f4f030df18a7147f30ee"},"source":["<a id=\"there_you_go_3.2\"></a>\n","## 3.2) Memory Based Collaborative Filtering"]},{"cell_type":"markdown","metadata":{"_uuid":"104b464ee6d44688f2c120193855179204592a75"},"source":["There are 2 approaches to Memory-Based CF -->\n","\n","1) **User-User Collaborative Filtering** - In this we we calculate similarity of all the users to the active user ( the user whom the prediction is for ).Then sort and filter the Top-N users to make predictions for the active user. This is usually very effective but takes a lot of time and resources. For example if Dennis and Davis like the same movies and a new movie comes out that Davis likes,then we can recommend that movie to Dennis because Davis and Dennis seem to like the same movies.\n","\n","2) **Item-Item Collaborative Filtering** - This is similar to User-User CF, just that we now compute similarity between items to recommend similar items. Eg. When you buy any product on Amazon, you will find this line \"Users who bought this item also bought...\", so Amazon uses item-item CF widely, Mind that I'm not saying they use only item-item CF, they have hybrid techniques to better suit users of even unique interests.\n","\n","Item-Item CF are a lot faster than User-User CF. and secondly user profiles changes quickly and the entire system model has to be recomputed, whereas item's average ratings doesn't change that quickly, and this leads to more stable rating distributions in the model, so the model doesn't have to be rebuilt as often."]},{"cell_type":"markdown","metadata":{"_uuid":"71a4c5d724ea1a62b7220c0b904b0ef24680979f"},"source":["**Q) How do we calculate similarity?**\n","\n","**Ans.** There are many measures to calculate the similarity matrix, some of them are -->\n","\n","1) **Jaccard Similarity** - It is a statistic used for comparing the similarity and diversity of sample sets. It is defined as the size of the intersection divided by the size of the union of the sample sets.\n","\n","2) **Cosine Similarity** - It measures the angle between the ratings vector. If the angle is 0Â°, then they are vectors having same orientation and if the angle is 180Â°, then they are highly dissimilar vectors.\n","\n","3) **Pearson Similarity** - It is actually Centered-Cosine similarity. We subtract the mean ratings from the user ratings, so that the mean is centered at 0, and then calculate the cosine similarity."]},{"cell_type":"markdown","metadata":{"_uuid":"82fef40ac1a4136eeeea55f5881335087ad571a3"},"source":["<a id=\"there_you_go_3.2.1\"></a>\n","### 3.2.1) User based Collaborative Filtering"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"38ae615711be06f6a1aa16770ddd6ead7919bb56","trusted":true},"outputs":[],"source":["user_util_matrix = util_mat.copy()\n","\n","# We will fill the row wise NaN's with the corresponding user's mean ratings, so that we can carry out Pearson correlation.\n","# Here we assume avg ratings for the movie that is not rated.\n","user_util_matrix = user_util_matrix.apply(lambda row: row.fillna(row.mean()), axis=1)\n","user_util_matrix.head(5)"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"96f2450c5cb20a39dcfc6449c9ea342c2f1e22e6","trusted":true},"outputs":[],"source":["user_util_matrix.T.corr()"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"dfce34de27d4353959ebd3251fcb379bc8163457","trusted":true},"outputs":[],"source":["user_corr_mat = user_util_matrix.T.corr()\n","corr_user_1 = user_corr_mat.iloc[0]"]},{"cell_type":"markdown","metadata":{"_uuid":"fc083259ff8f7b4bf2e39f72ecee268d629998ee"},"source":["For convinience, I will be be considering only the correlation of all users with the first user only."]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"5478a6bea4d6711b9a9791b199f695791c54590e","trusted":true},"outputs":[],"source":["corr_user_1.sort_values(ascending=False, inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"f375e6f2f18dbf23544582113752d733bf481689","trusted":true},"outputs":[],"source":["corr_user_1"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"9bf4e612acaf357f1b200a3ab6a61d5dcc31b069","trusted":true},"outputs":[],"source":["# NaN values are generated in corr() as the std dev is zero, which is required in calculating Pearson Similarity.\n","corr_user_1.dropna(inplace = True)"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"76358689a137051ba86eacff4f07af19bd0a141f","trusted":true},"outputs":[],"source":["# Neglect the 1st corr value as it is user1 itself\n","top50_corr_users = corr_user_1[1:51]"]},{"cell_type":"markdown","metadata":{"_uuid":"151a8e82a6c2b4b8dfb424a411dda175612e0099"},"source":["Below, we have list of all movies that user 1 has ever rated."]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"508eb4150c3b0697f8e017ee9fc10d1b0c5b432b","trusted":true},"outputs":[],"source":["df_combined[ df_combined['userId'] == 1]"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"6509e470f100d49ecd0748a4a583f530a473d5f8","trusted":true},"outputs":[],"source":["# user1 has not rated 32 movie\n","df_combined[ (df_combined['userId'] == 1) & (df_combined['movieId'] == 32) ] "]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"73695b0c25ab08f66a0347c2703a959633b48c92","trusted":true},"outputs":[],"source":["print('32nd Movie : ', movies['title'][ movies['movieId'] == 32 ].values)"]},{"cell_type":"markdown","metadata":{"_uuid":"9ae02f0c35676058bfcd618b7d1971b2e9f1a0af"},"source":["So, let's calculate what ratings user 1 would give to the movie with the help of similarrity vector. And based on that rating, we can compare it with a threshold rating. If the rating is higher it will be visible to the active user in his/her recommended list."]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"03956bd9ae1ad18af7e16f7008b83e0e01e46b9a","trusted":true},"outputs":[],"source":["df_n_ratings.loc[['Twelve Monkeys (a.k.a. 12 Monkeys) (1995)']]"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"bf2ca5d01c83ddb6c155e90a035d24266ae40967","trusted":true},"outputs":[],"source":["top50_users = top50_corr_users.keys()\n","\n","count = 0\n","users = list()\n","for user in top50_users:\n","    if df_combined[ (df_combined['userId'] == user) & (df_combined['movieId'] == 32) ]['rating'].sum()  :\n","        count +=1\n","        users.append(user)\n","\n","print(count)"]},{"cell_type":"markdown","metadata":{"_uuid":"73e66b57828d0a0923e010b7a25d8539459d3668"},"source":["There are 30 similar users among the Top-50 similar users that have rated the movie \"The Twelve Monkeys\".\n","\n","* Now, let's calculate the rating user 1 would give to the movie, \n","\n","* **Predicted rating** = sum of [ (weights) * (ratings) ]  **/** sum of  (weights)\n","\n","Here, *weights* is the correlation of the corresponding user with the first user.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"dbf19a02eac8489cabd89ce83daf841a78ec956e","trusted":true},"outputs":[],"source":["# Let's define a function to calculate what user1 will rate the movie\n","# We use Weighted average of k similar users\n","\n","def predict_rating():\n","    sum_similarity = 0\n","    weighted_ratings = 0\n","    for user in users:\n","        weighted_ratings += top50_corr_users.ix[user] * df_combined[ (df_combined['userId'] == user) & \n","                                                              (df_combined['movieId'] == 32) ]['rating'].sum()\n","        sum_similarity += top50_corr_users.ix[user]\n","\n","    print(weighted_ratings / sum_similarity)\n","    \n","    \n","predict_rating()"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"9a43ee866ebbd94da8defbc4b252aabc48f20c47","trusted":true},"outputs":[],"source":["df_m[ df_m['movieId'] == 32]"]},{"cell_type":"markdown","metadata":{"_uuid":"30a998666b99034875994218079be771340114f0"},"source":["Well there are pretty good changes of recommending this movie movie to the 1st user, since the rating is quite good.\n","\n","So, this is how a naive User-based CF works. Predicted ratings are calculated similarly for every user, (obviously for the movies he's not rated) and depending upon the threshold rating, the movie is either displayed on his recommended list or discarded."]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"783a1e8eb5c54ae8ba5a19e2d85fdf9301c7f1a5","trusted":true},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"_uuid":"34f1b131d985b318b02e106148e16b551c44f541"},"source":["<a id=\"there_you_go_3.2.2\"></a>\n","### 3.2.2) Item Based Collaborative Filtering\n","\n","\n","* It is quite similar to previous algorithm, but instead of finding user's look-alike, we try finding movie's look-alike. \n","* Once we have movie's look-alike matrix, we can easily recommend alike movies to user who have rated any movie from the dataset.\n","\n","![Imgur](https://i.imgur.com/wKMnQiU.jpg)"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"e188f0797356d83f60ecfe87b08a456a696e312e","trusted":true},"outputs":[],"source":["# Let's find similar movies to jurassic Park\n","df_n_ratings.loc[['Jurassic Park (1993)']]"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"acfecb492ed23b6d5ec21c5cb6a07f1ce8074eab","trusted":true},"outputs":[],"source":["item_util_matrix = util_mat.copy()\n","item_util_matrix.head(10)"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"669bf2591d64d3a80b7e895e63ec4d8bede667a0","trusted":true},"outputs":[],"source":["# We will fill the column wise NaN's with the corresponding movie's mean ratings, so that we can carry out Pearson correlation.\n","# Here we assume avg ratings for the user that has not a rated movie.\n","\n","item_util_matrix = item_util_matrix.apply(lambda col : col.fillna(col.mean()), axis=0)\n","item_util_matrix.head(5)"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"392dd321fdbe3cb448bd727f0b154d79e7d1feb5","trusted":true},"outputs":[],"source":["item_util_matrix.isna().sum().sum()"]},{"cell_type":"markdown","metadata":{"_uuid":"a2afd1c644d59e18a80e17eef20e4f2f839a3f26"},"source":["This signifies that every Movie is rated by atleast 1 user."]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"d76ac6a96895185ab7ec02555ed9e043eaecccd2","trusted":true},"outputs":[],"source":["item_util_matrix.corr()"]},{"cell_type":"markdown","metadata":{"_uuid":"168a49cf049528b49cf2bdc2a096efb127cf15c0"},"source":["* There are lot of NaN values and that is because when we are calculating the Pearson correlation, if the rating vector has all the values same for eg -> [3.0 , 3.0, 3.0, 3.0, ....], then the **Standard Deviation** is zero and division by zero is undefined, and thus its correlation with any other rating vector is NaN.\n","\n","* Since there are many movies that are rated only by 1 user , there the whole column mean is filled with the rating of that user, and therefore it's Pearson correlation gives NaN values with any other column."]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"c1609da96c0e281fdba2a31ceacde92b19bf0d95","trusted":true},"outputs":[],"source":["item_corr_matrix = item_util_matrix.corr()"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"1a0b2bc73139ff14d1736cfafd96a5eed9a65534","trusted":true},"outputs":[],"source":["jurassic_park_corr = item_corr_matrix['Jurassic Park (1993)']\n","jurassic_park_corr = jurassic_park_corr.sort_values(ascending=False)\n","jurassic_park_corr.dropna(inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"580e0de18bb414166273d1f20e68e5533103cd8a","trusted":true},"outputs":[],"source":["movies_similar_to_jurassic_park = pd.DataFrame(data=jurassic_park_corr.values, columns=['Correlation'], \n","                                               index = jurassic_park_corr.index)\n","movies_similar_to_jurassic_park = movies_similar_to_jurassic_park.join(df_n_ratings['total ratings'])\n","movies_similar_to_jurassic_park.head(10)"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"786d7fe64730c72a1f2ae15b4713f9732b3205c0","trusted":true},"outputs":[],"source":["movies_similar_to_jurassic_park = movies_similar_to_jurassic_park[1:]\n","movies_similar_to_jurassic_park[ movies_similar_to_jurassic_park['total ratings'] > 100 ].sort_values(ascending=False,\n","                                                                                          by=['Correlation']).head(10)"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"21ebcaf33da5d820195c38e2578aaca04e5d22a0","trusted":true},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"_uuid":"ae4476ba564b485600e9123ae57aa5b6fdfb812f"},"source":["<a id=\"there_you_go_3.3\"></a>\n","## 3.3) Model Based Collaborative Filtering"]},{"cell_type":"markdown","metadata":{"_uuid":"064b203ebb86ffdede5677e4cf9426447ceadc06"},"source":["Here we will be using dimensionality reduction methods to improve robustness and accuracy of Memory-Based CF. Basically, we compress user-item matrix into a low dimension matrix. We use techniques like SVD which is a low-rank factorization method, PCA which is used for dimensionaliry reduction etc.\n","\n","Model-based methods are based on matrix factorization and are better at dealing with sparsity. \n","* We will be using a \"Surprise\" library to implement SVD, KNN and NMF.\n","* You can find its documentation here >  [https://surprise.readthedocs.io/en/stable/](https://surprise.readthedocs.io/en/stable/)\n","* Surprise Library has almost all the algorithms implemented that are required for model-based Recommendation systems."]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"0e3611072c125c64152237b86c355ac22754ae23","trusted":true},"outputs":[],"source":["from surprise import Reader, Dataset, evaluate, KNNBasic, SVD, NMF\n","from surprise.model_selection import GridSearchCV, cross_validate"]},{"cell_type":"markdown","metadata":{"_uuid":"a44b128f8dfb546899d4c1bd4b27013ea050d6a3"},"source":["To load a dataset from a pandas dataframe, you will need the **load_from_df()** method. You will also need a **Reader** object, but only the rating_scale parameter must be specified.\n","\n","The Reader class is used to parse a file containing ratings."]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"0bf01bbc08a439cce0bbf1ed60b13e21a47559ff","trusted":true},"outputs":[],"source":["reader = Reader(rating_scale=(0.5, 5.0))\n","\n","data = Dataset.load_from_df( ratings[['userId', 'movieId', 'rating']], reader = reader )"]},{"cell_type":"markdown","metadata":{"_uuid":"1207eb620682aad721b471f7a4c60b070cfcb525"},"source":["<a id=\"there_you_go_3.3.1\"></a>\n","### 3.3.1) K-Nearest Neighbours (KNN)"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"a1808c50c2184758606f7bfe6255721cb5c43ac0","trusted":true},"outputs":[],"source":["# Split data into k-folds\n","# data.split(n_folds=5)"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"c7851ba00d89084bbfdf5a141ca60231a855b6dc","trusted":true},"outputs":[],"source":["# Compute Mean Squared Distance Similarity\n","sim_options = {'name' : 'msd'}\n","\n","algo = KNNBasic(k=20, sim_options=sim_options )\n","cross_validate(algo=algo, data=data, measures=['RMSE'], cv=5, verbose=True)"]},{"cell_type":"markdown","metadata":{"_uuid":"36bd64d20d57e6d57709f98eb8d6b380cd49894c"},"source":["**Tuning KNN using GridSearchCV**"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"8cdc6cc95681aa214fb1631da7a9810576cabd00","trusted":true},"outputs":[],"source":["n_neighbours = [10, 20, 30]\n","param_grid = {'n_neighbours' : n_neighbours}\n","\n","gs = GridSearchCV(KNNBasic, measures=['RMSE'], param_grid=param_grid)\n","gs.fit(data)\n","\n","print('\\n\\n###############')\n","# Best RMSE score\n","print('Best Score :', gs.best_score['rmse'])\n","\n","# Combination of parameters that gave the best RMSE score\n","print('Best Parameters :', gs.best_params['rmse'])\n","print('###############')"]},{"cell_type":"markdown","metadata":{"_uuid":"f472339bd459eed99690d0b825e9cecc53cdd6c2"},"source":["<a id=\"there_you_go_3.3.2\"></a>\n","### 3.3.2) Singular Value Decomposition (SVD)"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"a56b1de2fa83fa7a5e5096ff8e82ab18daa4fc83","trusted":true},"outputs":[],"source":["algo = SVD()\n","cross_validate(algo=algo, data=data, measures=['RMSE'], cv=5, verbose=True)"]},{"cell_type":"markdown","metadata":{"_uuid":"9e4b565ea64505d6344dc7720b49bbb4597e672c"},"source":["**Fine Tuning SVD using GridSearchCV**\n","\n","[Click here to view the parameters](https://surprise.readthedocs.io/en/stable/matrix_factorization.html)"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"80b0898392b78475299fa0e57b0c529147e7e02b","trusted":true},"outputs":[],"source":["param_grid = {'n_factors' : [50, 75], 'lr_all' : [0.5, 0.05], 'reg_all' : [0.06, 0.04]}\n","\n","gs = GridSearchCV(algo_class=SVD, measures=['RMSE'], param_grid=param_grid)\n","gs.fit(data)\n","\n","print('\\n###############')\n","# Best RMSE score\n","print('Best Score :', gs.best_score['rmse'])\n","\n","# Combination of parameters that gave the best RMSE score\n","print('Best Parameters :', gs.best_params['rmse'])\n","print('###############')"]},{"cell_type":"markdown","metadata":{"_uuid":"a6e2139dd63e0dd6eb2749022301d3e1fa59193b"},"source":["<a id=\"there_you_go_3.3.3\"></a>\n","### 3.3.3) Non-Negative Matrix Factorization (NMF)"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"8b6b979d7ea7448ecaf3bb29b4d83fdfdf63ba97","trusted":true},"outputs":[],"source":["algo = NMF()\n","cross_validate(data=data, algo=algo, measures=['RMSE'], cv=5, verbose=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"c2474a77d53e5ade18122c1fa1a19e9ebee458a9","trusted":true},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"_uuid":"ce42dd608233fb22615f05c702fc9eb20f3ce4ed"},"source":["<a id=\"there_you_go_4\"></a>\n","# 4) Matrix Factorization using Deep Learning (Keras)"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"8368a5686303cd2079f4abc987dfbc972aab477a","trusted":true},"outputs":[],"source":["from keras.layers import Embedding, Input, dot, concatenate\n","from keras.models import Model\n","from IPython.display import SVG\n","from keras.utils.vis_utils import model_to_dot"]},{"cell_type":"markdown","metadata":{"_uuid":"fb98e82422fe3453d62918952cba0c30cbbaff4e"},"source":["<a id=\"there_you_go_4.1\"></a>\n","## 4.1)Splitting Data into Train and Validation Set"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"6ddeb883e5fe1879c6c08b98bdd0967d556d0b6f","trusted":true},"outputs":[],"source":["X = ratings.iloc[:,:2]\n","Y = ratings.iloc[:,2]\n","\n","x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2, random_state = 66)"]},{"cell_type":"markdown","metadata":{"_uuid":"7f128e596382ff4165b63523d01275af6d708dff"},"source":["<a id=\"there_you_go_4.2\"></a>\n","## 4.2) Building the Model using Embedding Layers\n","\n","An embedding is a mapping from discrete objects, such as words or movies in our case, to a vector of continuous values. These are used to find similarities between discrete objects. \n","\n","The concept behind matrix factorization models is that the preferences of a user can be determined by a small number of hidden factors. And these are called as **Embeddings**.\n","\n","![Imgur](https://i.imgur.com/zGQJFLD.png)\n","\n","As you can see in the image, there are 2 features both for user and the items. These are the latent factors or the hidden factors. These factors have a value for the corresponding user and determines to what extend that user likes the feature.\n","\n","For Eg. the features for the user could be ->\n","* How much he likes action movies?\n","* Whether he likes old movies?\n","\n","And the features for the movies could be ->\n","* To what scale is it an action movie?\n","* Whether the movie is recently released?\n","\n","And finally we take **dot product** which gives us the user's rating for the movie, \n","\n","**Estimated Rating** = [ (How much he likes action movies?) x (To what scale is it an action movie) ]  +  [ (Whether he likes old movies?) x (Whether the movie is recently released) ]"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"79c2c73e0e8ac1e0dd35c48acedfe4840329e759","trusted":true},"outputs":[],"source":["# The number of latent factors for the embedding\n","n_latent_factors = 50\n","\n","# no of users and movies\n","n_users, n_movies = len(ratings['userId'].unique()), len(ratings['movieId'].unique()) "]},{"cell_type":"markdown","metadata":{"_uuid":"247040fe84e0161a6002b11213a799acbcf48567"},"source":["<a id=\"there_you_go_4.3\"></a>\n","## 4.3) Architecture 1"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"207734899b02951feb88e6f7add7393414fe8c7a","trusted":true},"outputs":[],"source":["# Model Architecture\n","\n","\n","# User Embeddings\n","user_input = Input(shape=(1,), name='User_Input')\n","user_embeddings = Embedding(input_dim = n_users, output_dim=n_latent_factors, input_length=1, \n","                              name='User_Embedding') (user_input)\n","user_vector = Flatten(name='User_Vector') (user_embeddings)\n","\n","\n","# Movie Embeddings\n","movie_input = Input(shape=(1,), name='Movie_Input')\n","movie_embeddings = Embedding(input_dim = n_movies, output_dim=n_latent_factors, input_length=1, \n","                               name='Movie_Embedding') (movie_input)\n","movie_vector = Flatten(name='Movie_Vector') (movie_embeddings)\n","\n","\n","# Dot Product\n","merged_vectors = dot([user_vector, movie_vector], name='Dot_Product', axes=1)\n","model = Model([user_input, movie_input], merged_vectors)"]},{"cell_type":"markdown","metadata":{"_uuid":"5f12876e26b764b1f8abe1256a290df21af2bc28"},"source":["<a id=\"there_you_go_4.4\"></a>\n","## 4.4) Visualizing the Model Architecture"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"2fddfaba193fdf928910665fbc1b406f6c466310","trusted":true},"outputs":[],"source":["SVG(model_to_dot( model,  show_shapes=True, show_layer_names=True).create(prog='dot', format='svg'))"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"d8a7cc21d7625d6d4b6c4563f3eef637ad88e0d6","trusted":true},"outputs":[],"source":["model.summary()"]},{"cell_type":"markdown","metadata":{"_uuid":"89d4a53b62018704b524b2653de6650b87bad224"},"source":["\n","<a id=\"there_you_go_4.5\"></a>\n","## 4.5) Compiling the Model"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"b499d9b0492c7c59e0696c816c0f35e6c7fd9665","trusted":true},"outputs":[],"source":["optimizer = Adam(lr = 0.0005)\n","\n","model.compile(loss='mean_squared_error', optimizer = optimizer)"]},{"cell_type":"markdown","metadata":{"_uuid":"78572c2c1b376327d3e8cdc38df16b5339c94392"},"source":["<a id=\"there_you_go_4.6\"></a>\n","## 4.6) Fitting the Model"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"5263657ef6f47f98c63cb8a89ffe38740c7db561","trusted":true},"outputs":[],"source":["batch_size = 128\n","epochs = 20"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"7b974f90f2c029a094f0fe207c30fbdd07835a19","trusted":true},"outputs":[],"source":["history = model.fit(x=[x_train['userId'], x_train['movieId']], y=y_train, batch_size= batch_size, epochs=epochs, \n","                    verbose= 2, validation_data=([x_test['userId'], x_test['movieId']], y_test))"]},{"cell_type":"markdown","metadata":{"_uuid":"f16afd148fdf381bd40f21fc8580b58f28ad1f22"},"source":["<a id=\"there_you_go_4.7\"></a>\n","## 4.7) Plotting Validation Curves"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"2bc86f3cf8e6e462090564ec4ad776273ba6f4c4","trusted":true},"outputs":[],"source":["# Get training and test loss histories\n","training_loss = history.history['loss']\n","test_loss = history.history['val_loss']\n","\n","# Create count of the number of epochs\n","epoch_count = range(1, len(training_loss) + 1)\n","\n","# Visualize loss history\n","plt.figure(figsize = (8,4))\n","plt.plot(epoch_count, training_loss, 'r--')\n","plt.plot(epoch_count, test_loss, 'b-')\n","plt.legend(['Training Loss', 'Test Loss'])\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"_uuid":"4e7c5b699c6d3971a93c74abfe8525a6bde4c96b"},"source":["<a id=\"there_you_go_4.\"></a>\n","## 4.8) Evaluating RMSE"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"4fa0deab0c21c0ddd328f7fcb81d31340b136d8c","trusted":true},"outputs":[],"source":["score = model.evaluate([x_test['userId'], x_test['movieId']], y_test)\n","print()\n","print('RMSE: {:.4f}'.format(np.sqrt(score)))"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"2e97eaba3c2b57e1398dbdca5b4374a2b2affb3e","trusted":true},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"_uuid":"a611ec33e331eec0cd2552e8779b56144416c3fe"},"source":["<a id=\"there_you_go_4.9\"></a>\n","## 4.9) Architecture 2\n","In this architecture we will be concatenating the latent factors and use Dense layers to calculate the predicted ratings. \n","\n","Have a look at the image below ->\n","![Imgur](https://i.imgur.com/GIe3gTZ.png)"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"d1b5338b38493f9eb74972306794f4dc921315ef","trusted":true},"outputs":[],"source":["# Model Architecture\n","\n","\n","# User Embeddings\n","user_input = Input(shape=(1,), name='User_Input')\n","user_embeddings = Embedding(input_dim = n_users, output_dim=n_latent_factors, input_length=1, \n","                              name='User_Embedding') (user_input)\n","user_vector = Flatten(name='User_Vector') (user_embeddings)\n","\n","\n","\n","# Movie Embeddings\n","movie_input = Input(shape=(1,), name='Movie_Input')\n","movie_embeddings = Embedding(input_dim = n_movies, output_dim=n_latent_factors, input_length=1, \n","                               name='Movie_Embedding') (movie_input)\n","movie_vector = Flatten(name='Movie_Vector') (movie_embeddings)\n","\n","\n","\n","\n","# Concatenate Product\n","merged_vectors = concatenate([user_vector, movie_vector], name='Concantenate')\n","dense_layer_1 = Dense(100, activation='relu')(merged_vectors) \n","# dense_layer_1 = Dropout(0.25) (dense_layer_1)\n","# batchnorm_layer_1 = BatchNormalization()(dense_layer_1)\n","# dense_layer_2 = Dense(64, activation='relu')(merged_vectors)\n","\n","\n","result = Dense(1)(dense_layer_1)\n","model = Model([user_input, movie_input], result)"]},{"cell_type":"markdown","metadata":{"_uuid":"20f8edc7eca8003c9c7c6c866a7c32f2fd66ee0e"},"source":["<a id=\"there_you_go_4.10\"></a>\n","## 4.10) Visualizing the Model Architecture"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"ac7b1f2fc3cc7e02e1de677ae63d3845e0750ce9","trusted":true},"outputs":[],"source":["SVG(model_to_dot( model,  show_shapes=True, show_layer_names=True).create(prog='dot', format='svg'))"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"a5719bb8ba6c2a03f9a4cd7cdd293ebf94a6ff87","trusted":true},"outputs":[],"source":["model.summary()"]},{"cell_type":"markdown","metadata":{"_uuid":"331d31a25df4513629e5921a92f147538b2db770"},"source":["<a id=\"there_you_go_4.11\"></a>\n","## 4.11) Compiling the Model"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"cf43f5b99b030c0281b8331789c9d9a2dc1a3798","trusted":true},"outputs":[],"source":["optimizer = Adam(lr=0.0002)"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"2e3f9ef79fbef9798e42aeaba4e0c7720ab189d5","trusted":true},"outputs":[],"source":["model.compile(loss='mean_squared_error', optimizer=optimizer)"]},{"cell_type":"markdown","metadata":{"_uuid":"0096da22916b4714d98df9c254c2e14de67cd46d"},"source":["<a id=\"there_you_go_4.12\"></a>\n","## 4.12) Fitting the Model"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"45445039d83b70b00afaf3d3b47037ad14f6e947","trusted":true},"outputs":[],"source":["batch_size = 128\n","epochs = 20"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"4fa96023bcb4b56835003bc29eb0362d34903048","trusted":true},"outputs":[],"source":["history = model.fit(x=[x_train['userId'], x_train['movieId']], y=y_train, batch_size= batch_size, epochs=epochs, \n","                    verbose= 2, validation_data=([x_test['userId'], x_test['movieId']], y_test))"]},{"cell_type":"markdown","metadata":{"_uuid":"b31a012b86d17c819d16ebb966aedcfe39050853","trusted":true},"source":["<a id=\"there_you_go_4.13\"></a>\n","## 4.13) Plotting Validation Curves"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"8435fd0af309608e37ffb6bcb775757ac1d4c0e4","scrolled":true,"trusted":true},"outputs":[],"source":["# Get training and test loss histories\n","training_loss = history.history['loss']\n","test_loss = history.history['val_loss']\n","\n","# Create count of the number of epochs\n","epoch_count = range(1, len(training_loss) + 1)\n","\n","# Visualize loss history\n","plt.figure(figsize = (8,4))\n","plt.plot(epoch_count, training_loss, 'r--')\n","plt.plot(epoch_count, test_loss, 'b-')\n","plt.legend(['Training Loss', 'Test Loss'])\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"_uuid":"8ee3d4045052b3f991b216954d58376771f06feb"},"source":["<a id=\"there_you_go_4.14\"></a>\n","## 4.14) Evaluating RMSE"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"5772d56a06df2a89602e26c744d9c7e5c3658a0a","trusted":true},"outputs":[],"source":["score = model.evaluate([x_test['userId'], x_test['movieId']], y_test)\n","\n","print()\n","print('RMSE: {:.4f}'.format(np.sqrt(score)))"]},{"cell_type":"markdown","metadata":{"_uuid":"a15316487e0e9b6f59c8b720a433d5b2b95cf7cb","trusted":true},"source":["So, as you could see the latter architecture did a pretty good job in predicting the user ratings. You can try tuning the number of latent factors, also try changing the architecture, adding more layers (make sure it doesn't overfit) and introducing Dropout and BatchNorm layers to acheive even lower RMSE."]},{"cell_type":"markdown","metadata":{"_uuid":"6b5fdda29c5a20b8ffdb595451a2b7f4c2b87735"},"source":["<a id=\"there_you_go_5\"></a>\n","# 5) References\n","1.  Linear Algebra - [Good for quick revision](https://www.kaggle.com/mjbahmani/linear-algebra-for-data-scientists)\n","2. Stanford lectures - [Starts at Lecure 41 and you can proceed as much you need, covers CF, SVD and much more](https://www.youtube.com/watch?v=1JRrCEgiyHM&index=41&list=PLLssT5z_DsK9JDLcT8T62VtzwyW9LNepV)\n","3. Blogs - [Various Implementations of Collaborative Filtering](https://towardsdatascience.com/various-implementations-of-collaborative-filtering-100385c6dfe0)\n","4. Deep Learning Blog - [Implementing recommendation systems in keras](https://nipunbatra.github.io/blog/2017/recommend-keras.html)\n","5. Matrix Factorization - [Understanding how Matrix Factorization works](https://lazyprogrammer.me/tutorial-on-collaborative-filtering-and-matrix-factorization-in-python/)\n","6. Surprise Package - [Library to implement Model-Based CF Algorithms](https://surprise.readthedocs.io/en/stable/)"]},{"cell_type":"markdown","metadata":{"_uuid":"e328d29d2b8abe726e318b1d25d085eb1e126608"},"source":["*Thank you...*\n","\n","*Do star/upvote if you like it ;)*"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"097a187c90edc2aded7b60387308862ae9bffedb","trusted":true},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":98156,"sourceId":230378,"sourceType":"datasetVersion"}],"dockerImageVersionId":29282,"isGpuEnabled":true,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"}},"nbformat":4,"nbformat_minor":4}
